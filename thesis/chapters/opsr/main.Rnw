%% -- Introduction -------------------------------------------------------------

%% - In principle "as usual".
%% - But should typically have some discussion of both _software_ and _methods_.
%% - Use \proglang{}, \pkg{}, and \code{} markup throughout the manuscript.
%% - If such markup is in (sub)section titles, a plain text version has to be
%%   added as well.
%% - All software mentioned should be properly \cite-d.
%% - All abbreviations should be introduced.
%% - Unless the expansions of abbreviations are proper names (like "Journal
%%   of Statistical Software" above) they should be in sentence case (like
%%   "generalized linear models" below).

\section{Introduction} \label{sec:intro}

The goal of the program evaluation literature is to estimate the effect of a treatment program (e.g., a new policy, technology, medical treatment, or agricultural practice) on an outcome. To evaluate such a program, the ``treated'' are compared to the ``untreated''. In an experimental setting, the treatment can be (randomly) assigned by the researcher. However, in an observational setting, the treatment is not always exogenously prescribed but rather self-selected. This gives rise to a selection bias when unobserved factors influencing the treatment adoption also influence the outcome (also known as \emph{selection on unobservables}). Simple group comparison no longer yield an unbiased estimate of the treatment effect. In more technical terms, the counterfactual outcome of the treated (``if they had not been treated'') does not necessarily correspond to the factual outcome of the untreated. For example, cyclists riding without a helmet (the ``untreated'') might have a risk-seeking tendency. We therefore potentially overestimate the benefit of wearing a helmet if we compare the accident (severity) rate of the two groups. Risk-seeking is not readily measured and it is easy to imagine that it becomes part of the error in applied research and thus leading cause of a selection bias.

To properly account for the selection bias, various techniques exist, both for longitudinal and cross-sectional data. In the first case, difference in differences is a widely adopted measure. In the latter case, instrumental variables, matching propensity scores, regression-discontinuity design, and the endogenous switching regression model have been applied \citep{Wang+Mokhtarian:2024}. The latter method is particularly well-suited to correct for both selection on observables and unobservables (unlike other methods which only address and correct for selection on observables).

The seminal work by \cite{Heckman:1979} proposed a two-part model to address the selection bias that often occurs when modelling a continuous outcome which is only observable for a subpopulation. A very nice exposition of this model is given in \citet[][Chapter~16]{Cameron+Trivedi:2005}. The classical Heckman model consists of a probit equation and continuous outcome equation. A natural extension is then switching regression, where the population is partitioned into different groups (regimes) and separate parameters are estimated for the continuous outcome process of each group. This model is originally known as the Roy model \citep{Cameron+Trivedi:2005} or Tobit 5 model \citep{Amemiya:1985}. These classical models (the Tobit models for truncated, censored or interval data and their extensions) are implemented in various environments for statistical computing and in \proglang{R}'s \citep{R} \pkg{sampleSelection} package \citep{Toomet+Henningsen:2008}.

Many different variants can then be derived by either placing different distributional assumptions on the errors and/or how the latent process manifests into observed outcomes (i.e., the dependent variables can be of various types, such as binary, ordinal, censored, or continuous) more generally known as conditional mixed-process (CMP) models. CMP models comprise a broad family involving two or more equations featuring a joint error distribution assumed to be multivariate normal. The \proglang{Stata} \citep{Stata} command \code{cmp} \citep{Roodman:2011} can fit such models. The variant at the heart of this paper is an ordered probit switching regression (OPSR) model, with ordered treatments and continuous outcome. Throughout the text we use the convention that OPSR refers to the general methodology, while \pkg{OPSR} refers specifically to the package.

OPSR is available as a \proglang{Stata} command, \code{oheckman} \citep{Chiburis+Lokshin:2007}, which however, does not allow distinct specifications for the continuous outcome processes (i.e., the same explanatory variables must be used for all treatment groups). The relatively new \proglang{R} package \pkg{switchSelection} \citep{Potanin:2024} allows to estimate multivariate and multinomial sample selection and endogenous switching models with multiple outcomes. These models are systems of ordinal, continuous and multinomial equations and thus nest OPSR as a special case.

\pkg{OPSR} is tailored to one particular method, easy to use (understand, extend and maintain), fast and memory efficient. Unlike the mentioned implementations, it handles log-transformed continuous outcomes which need special consideration for the computation of conditional expectations. It obeys to \proglang{R}'s implicit modeling conventions (by extending the established generics such as \fct{summary}, \fct{predict}, \fct{update}, \fct{anova} among others) and produces production-grade output tables. This work generalizes the learnings from \cite{Wang+Mokhtarian:2024} and makes the OPSR methodology readily available. The mathematical notation presented here translates to code almost verbatim which hopefully serves a pedagogical purpose for the curious reader.

The remainder of this paper is organized as follows: Section~\ref{sec:model} outlines the ordered probit switching regression model, lists all the key formulas underlying the software implementation and details \pkg{OPSR}'s architecture. In Section~\ref{sec:illustrations} the key functionality is demonstrated both on simulated data and the data from \cite{Wang+Mokhtarian:2024} which we use to reproduce their core model. The case study in Section~\ref{sec:case-study} leverages tracking data from the TimeUse+ study \citep{Winkler+Meister+Axhausen:2024} investigating telework treatment effects on weekly distance traveled. There, we also compare the OPSR models to the ones not accounting for error correlation and discuss the implications for treatment effects. The summary in Section~\ref{sec:summary} concludes.


%% -- Manuscript ---------------------------------------------------------------

%% - In principle "as usual" again.
%% - When using equations (e.g., {equation}, {eqnarray}, {align}, etc.
%%   avoid empty lines before and after the equation (which would signal a new
%%   paragraph.
%% - When describing longer chunks of code that are _not_ meant for execution
%%   (e.g., a function synopsis or list of arguments), the environment {Code}
%%   is recommended. Alternatively, a plain {verbatim} can also be used.
%%   (For executed code see the next section.)

\section{Model and software} \label{sec:model}

In the following, we outline the ordered probit switching regression model as well as list all the key formulas underlying the software implementation. \pkg{OPSR} follows the \proglang{R}-typical formula interface to a workhorse fitter function. Its architecture is detailed after the mathematical part.

As alluded, OPSR is a two-step model: One process governs the ordinal outcome and separate processes (for each ordinal outcome) govern the continuous outcomes. The ordinal outcome can also be thought of as a regime or treatment. In the subsequent exposition, we will refer to the two processes as \emph{selection} and \emph{outcome} process.

We borrow the notation from \cite{Wang+Mokhtarian:2024} where also all the derivations are detailed. For a similar exhibition, \citet{Chiburis+Lokshin:2007} can be consulted. Individual subscripts are suppressed throughout, for simplicity.

Let $\mathcal{Z}$ be a latent propensity governing the selection outcome
%
\begin{equation} \label{eq:selection}
\mathcal{Z} = \Wg + \epsilon,
\end{equation}
%
where $\boldsymbol{W}$ represents the vector of attributes of an individual, $\boldsymbol{\gamma}$ is the corresponding vector of parameters and $\epsilon \sim \mathcal{N}(0, 1)$ a normally distributed error term.

As $\mathcal{Z}$ increases and passes some unknown but estimable thresholds, we move up from one ordinal treatment to the next higher level
%
\begin{equation} \label{eq:thresholds}
Z = j \quad \mathrm{if}\ \kappa_{j-1} < \mathcal{Z} \le \kappa_j,
\end{equation}
%
where $Z$ is the observed ordinal selection variable, $j = 1, \dots, J$ indexes the ordinal levels of $Z$, and $\kappa_j$ are the thresholds (with $\kappa_0 = -\infty$ and $\kappa_J = \infty$). Hence, there are $J-1$ thresholds to be estimated. The probability that an individual self-selects into treatment group $j$ is
%
\begin{equation} \label{eq:prob-selection}
\begin{aligned}
\Prob[Z = j] &= \Prob[\kappa_{j-1} < \mathcal{Z} \le \kappa_j] \\
&= \Prob[\kappa_{j-1} - \Wg < \epsilon \le \kappa_j - \Wg] \\
&= \Phi(\kappa_j - \Wg) - \Phi(\kappa_{j-1} - \Wg).
\end{aligned}
\end{equation}
%
where $\Phi(\cdot)$ is the cumulative distribution function of the standard normal distribution.

The outcome model for the \jth treatment group is expressed as
%
\begin{equation} \label{eq:outcome}
y_j = \Xb + \eta_j,
\end{equation}
%
where $y_j$ is the observed continuous outcome, $\boldsymbol{X_j}$ the vector of observed explanatory variables associated with the \jth outcome model, $\boldsymbol{\beta_j}$ is the vector of associated parameters, and $\eta_j \sim \mathcal{N}(0, \sigma_j^2)$ is a normally distributed error term. At this point it should be noted that $\boldsymbol{X_j}$ and $\boldsymbol{W}$ may share some explanatory variables but not all, due to identification problems otherwise \citep{Chiburis+Lokshin:2007}.

The key assumption of OPSR is now that the errors of the selection and outcome models are jointly multivariate normally distributed
%
\begin{equation} \label{eq:multi-norm}
\begin{pmatrix}
\epsilon \\
\eta_1 \\
\vdots \\
\eta_j \\
\vdots \\
\eta_J
\end{pmatrix}
\sim \mathcal{N}\left(
\begin{pmatrix}
0 \\
0 \\
\vdots \\
0 \\
\vdots \\
0
\end{pmatrix},
\begin{pmatrix}
1 & \rho_1 \sigma_1 & \cdots & \rho_j \sigma_j & \cdots & \rho_J \sigma_J \\
\rho_1 \sigma_1 & \sigma_2^2 \\
\vdots &  & \ddots \\
\rho_j \sigma_j & & & \sigma_j^2 \\
\vdots & & & & \ddots \\
\rho_J \sigma_J & & & & & \sigma_J^2
\end{pmatrix}
\right),
\end{equation}
%
where $\rho_j$ represents the correlation between the errors of the selection model ($\epsilon$) and the \jth outcome model ($\eta_j$). If the covariance matrix should be diagonal (i.e., no error correlation), no selection-bias exists and the selection and outcome models can be estimated separately.

As shown in \cite{Wang+Mokhtarian:2024}, the log-likelihood of observing all individuals self-selecting into treatment $j$ and choosing continuous outcome $y_j$ can be expressed as
%
\begin{multline} \label{eq:log-lik}
\ell(\theta \mid \boldsymbol{W}, \boldsymbol{X_j}) = \sum_{j = 1}^{J} \sum_{\{j\}}
\left\{
\ln\left[
\frac{1}{\sigma_j} \phi\left(\frac{y_j - \Xb}{\sigma_j}\right)
\right] \quad + \right. \\
\left. \ln\left[
\Phi\left(
\frac{\sigma_j (\kappa_j - \Wg) - \rho_j(y_j - \Xb)}{\sigma_j\sqrt{1 - \rho_j^2}}
\right) -
\Phi\left(
\frac{\sigma_j (\kappa_{j-1} - \Wg) - \rho_j(y_j - \Xb)}{\sigma_j\sqrt{1 - \rho_j^2}}
\right)
\right]
\right\}
\end{multline}
%
where $\sum_{\{j\}}$ means the summation of all the cases belonging to the \jth selection outcome, $\phi(\cdot)$ and $\Phi(\cdot)$ are the density and cumulative distribution function of the standard normal distribution.

The conditional expectation can be expressed as
%
\begin{equation} \label{eq:cond-exp}
\begin{aligned}
\E[y_j \mid Z = j] &= \Xb + \E[\eta_j \mid \kappa_{j-1} - \Wg < \epsilon \le \kappa_j - \Wg] \\
&= \Xb - \rho_j\sigma_j \frac{\phi(\kappa_j - \Wg) - \phi(\kappa_{j-1} - \Wg)}{\Phi(\kappa_j - \Wg) - \Phi(\kappa_{j-1} - \Wg)},
\end{aligned}
\end{equation}
%
where the fraction is the ordered probit switching regression model counterpart to the inverse Mills ratio (IMR) term of a binary switching regression model. We immediately see, that regressing $\boldsymbol{X_j}$ on $y_j$ leads to an omitted variable bias if $\rho_j \neq 0$ which is the root cause of the selection bias. However, the IMR can be pre-computed based on an ordered probit model and then included in the second stage regression, which describes the Heckman correction \citep{Heckman:1979}. It should be warned, that since the Heckman two-step procedure includes an estimate in the second step regression, the resulting OLS standard errors and heteroskedasticity-robust standard errors are incorrect \citep{Greene:2002}.

To obtain unbiased treatment effects, we must further evaluate the ``counterfactual outcome'', which reflects the expected outcome under a counterfactual treatment (i.e., for $j' \neq j$)
%
\begin{equation} \label{eq:counterfact-exp}
\begin{aligned}
\E[y_{j'} \mid Z = j] &= \Xbd + \E[\eta_{j'} \mid \kappa_{j-1} - \Wg < \epsilon \le \kappa_j - \Wg] \\
&= \Xbd - \rho_{j'}\sigma_{j'} \frac{\phi(\kappa_j - \Wg) - \phi(\kappa_{j-1} - \Wg)}{\Phi(\kappa_j - \Wg) - \Phi(\kappa_{j-1} - \Wg)}.
\end{aligned}
\end{equation}
%
Let's assume that $y_j = \ln(Y_j + 1)$ in the previous equations. I.e., the continuous outcome was log-transformed as is usual in regression analysis. We have to note, that in such cases the Equations~\ref{eq:cond-exp}-\ref{eq:counterfact-exp} provide the conditional expectation of the log-transformed outcome. Therefore we need to back-transform $Y_j = \exp(y_j) - 1$ which yields
%
\begin{equation} \label{eq:log-cond-exp}
\E[Y_j \mid Z = j] =
\exp\left(\Xb + \frac{\sigma_j^2}{2}\right)
\left[
\frac{\Phi(\kappa_j - \Wg - \rho_j\sigma_j) - \Phi(\kappa_{j-1} - \Wg - \rho_j\sigma_j)}
{\Phi(\kappa_j - \Wg) - \Phi(\kappa_{j-1} - \Wg)}
\right] - 1
\end{equation}
%
for the factual case, and
%
\begin{equation} \label{eq:log-counterfact-exp}
\E[Y_{j'} \mid Z = j] =
\exp\left(\Xbd + \frac{\sigma_{j'}^2}{2}\right)
\left[
\frac{\Phi(\kappa_j - \Wg - \rho_{j'}\sigma_{j'}) - \Phi(\kappa_{j-1} - \Wg - \rho_{j'}\sigma_{j'})}
{\Phi(\kappa_j - \Wg) - \Phi(\kappa_{j-1} - \Wg)}
\right] - 1
\end{equation}
%
for the counterfactual case \citep{Wang+Mokhtarian:2024}.

This concludes the mathematical treatment and we briefly outline \pkg{OPSR}'s architecture which can be conceptualized as follows:
\begin{itemize}
\item We provide the usual formula interface to specify a model. To allow for multiple parts and multiple responses, we rely on the \pkg{Formula} package \citep{Zeileis+Croissant:2010}.
\item After parsing the formula object, checking the user inputs and computing the model matrices, the Heckman two-step estimator is called in \fct{opsr\_2step} to generate reasonable starting values.
\item These are then passed together with the data to the basic computation engine \fct{opsr.fit}. The main estimates are retrieved using maximum likelihood estimation by passing the log-likelihood function \fct{loglik\_cpp} (Equation~\ref{eq:log-lik}) to \fct{maxLik} from the \pkg{maxLik} package \citep{Henningsen+Toomet:2011}.
\item All the above calls are nested in the main interface \fct{opsr} which returns an object of class \class{opsr}. Several methods then exist to post-process this object as illustrated below.
\end{itemize}

The likelihood function \fct{loglik\_cpp} is implemented in \proglang{C++} using \pkg{Rcpp} \citep{Edelbuettel+Balamuta:2018} and relying on the data types provided by \pkg{RcppArmadillo} \citep{Edelbuettel+Sanderson:2014}. Parallelization is available using \proglang{OpenMP}. This makes \pkg{OPSR} both fast and memory efficient (as data matrices are passed by reference).


%% -- Illustrations ------------------------------------------------------------

%% - Virtually all JSS manuscripts list source code along with the generated
%%   output. The style files provide dedicated environments for this.
%% - In R, the environments {Sinput} and {Soutput} - as produced by Sweave() or
%%   or knitr using the render_sweave() hook - are used (without the need to
%%   load Sweave.sty).
%% - Equivalently, {CodeInput} and {CodeOutput} can be used.
%% - The code input should use "the usual" command prompt in the respective
%%   software system.
%% - For R code, the prompt "R> " should be used with "+  " as the
%%   continuation prompt.
%% - Comments within the code chunks should be avoided - these should be made
%%   within the regular LaTeX text.

\section{Illustrations} \label{sec:illustrations}

We first illustrate how to specify a model using \pkg{Formula}'s extended syntax and simulated data. Then the main functionality of the package is demonstrated. We conclude this section by demonstrating some nuances, reproducing the core model of \cite{Wang+Mokhtarian:2024}.

Let us simulate date from an OPSR process with three ordinal outcomes and distinct design matrices $\boldsymbol{W}$ and $\boldsymbol{X}$ (where $\boldsymbol{X} = \boldsymbol{X_j} \ \forall{j}$) by
%
<<sim-dat>>=
sim_dat <- opsr_simulate()
dat <- sim_dat$data
head(dat)
@
%
where \code{ys} is the selection dependent variable (or treatment group), \code{yo} the outcome dependent variable and \code{xs} respectively \code{xo} the corresponding explanatory variables.

Models are specified symbolically. A typical model has the form \code{ys | yo ~ terms_s | terms_o1 | terms_o2 | ...} where the \code{|} separates the two responses and process specifications. If the user wants to specify the same process for all continuous outcomes, two processes are enough (\code{ys | yo ~ terms_s | terms_o}). Hence the minimal \fct{opsr} interface call reads
%
<<opsr>>=
fit <- opsr(ys | yo ~ xs1 + xs2 | xo1 + xo2, data = dat,
  printLevel = 0)
@
%
where \code{printLevel = 0} omits working information during maximum likelihood iterations.

As usual, the fitter function does the bare minimum model estimation while inference is performed in a separate call to
%
<<summary-xinyi>>=
summary(fit)
@
%
The presentation of the model results is fairly standard and should not warrant further explanation with the following exceptions
\begin{enumerate}
\item The number of regimes along absolute counts are reported.
\item Pseudo R-squared (EL) is based on the log-likelihood of the ``equally likely'' model, while Pseudo R-squared (MS) is based on the log-likelihood of the ``market share'' model. These indicators reflect the goodness of fit for the selection process. The multiple R-squared is reported for all continuous outcomes collectively and for the regimes separately in brackets. These indicators reflect the goodness of fit for the outcome processes.
\item Coefficient names are based on the variable names as passed to the formula specification, except that \code{"s_"} is prepended to the selection coefficients, \code{"o[0-9]_"} to the outcome coefficients and the structural components \code{"kappa", "sigma", "rho"} (aligning with the letters used in Equation~\ref{eq:log-lik}) are hard-coded (but can be over-written).
\item The coefficients table reports robust standard errors based on the sandwich covariance matrix as computed with help of the \pkg{sandwich} package \citep{Zeileis:2006}. \code{rob = FALSE} reports conventional standard errors.
\item Two Wald-tests are conducted. One, testing the null that all coefficients of explanatory variables are zero and two, testing the null that all error correlation coefficients (\code{rho}) are zero. The latter being rejected indicates that selection bias is an issue.
\end{enumerate}

A useful benchmark is always the null model with structural parameters only. The null model can be derived from an \class{opsr} model fit as follows
%
<<null-model>>=
fit_null <- opsr_null_model(fit, printLevel = 0)
@
%
A model can be updated as usual
%
<<update>>=
fit_intercept <- update(fit, . ~ . | 1)
@
%
where we have removed all the explanatory variables from the outcome processes.

Several models can be compared with a likelihood-ratio test using
%
<<anova>>=
anova(fit_null, fit_intercept, fit)
@
%
If only a single object is passed, then the model is compared to the null model. If more than one object is specified a likelihood ratio test is conducted for each pair of neighboring models. As expected, both tests reject the null.

Models can be compared side-by-side using the \pkg{texreg} package \citep{Leifeld:2013}, which also allows the user to build production-grade tables as illustrated later.
%
<<texreg>>==
texreg::screenreg(list(fit_null, fit_intercept, fit),
  include.pseudoR2 = TRUE, include.R2 = TRUE, single.row = TRUE)
@
%
Finally, the key interest of an OPSR study almost certainly is the estimation of treatment effects which relies on (counterfactual) conditional expectations as already noted in the mathematical exposition.
%
<<predict>>==
p1 <- predict(fit, group = 1, type = "response")
p2 <- predict(fit, group = 1, counterfact = 2, type = "response")
@
%
where \code{p1} is the result of applying Equation~\ref{eq:cond-exp} and \code{p2} is the counterfactual outcome resulting from Equation~\ref{eq:counterfact-exp}. The following \code{type} arguments are available
\begin{itemize}
\item \code{type = "response"}: Predicts the continuous outcome according to the Equations referenced above.
\item \code{type = "unlog-response"}: Predicts the back-transformed response if the continuous outcome was log-transformed according to Equations~\ref{eq:log-cond-exp}-\ref{eq:log-counterfact-exp}.
\item \code{type = "prob"}: Returns the probability vector of belonging to \code{group}.
\item \code{type = "mills"}: Returns the inverse Mills ratio.
\end{itemize}
Elements are \code{NA_real_} if the \code{group} does not correspond to the observed regime (selection outcome). This ensures consistent output length.

Now that the user understands the basic workflow, we illustrate some nuances by reproducing a key output of \cite{Wang+Mokhtarian:2024} where they investigate the treatment effect of telework (TW) on weekly vehicle miles driven. The data is attached, documented (\code{?telework_data}) and can be loaded by
%
<<telework-data>>=
data("telework_data", package = "OPSR")
@
%
<<hidden, echo=FALSE>>=
start <- c(
  1.2, 2.4,  # kappa 1 & 2
  0.2, 0.4, 0.1, 0.3, 0.3, 0.2, 0.1, 0.1, -0.1, 0.1, 0.1, 0.3, 0.1, 0.1,  # selection
  3.744, -0.208, 0.010, 0.000, -0.392, -0.019, 0.130, 0.010, 0.415, 0.494, 0.437, 0.186, 0.124, -0.240,  # outcome 1
  2.420, 0.224, 0.670, 0.445, 0.219, 0.824, 0.704, 0.164, -0.176, 0.171,  # outcome 2
  2.355, -0.375, 0.476, 0.317, 0.187, 0.290, 0.313, 0.856, 0.248, -0.275,  # outcome 3
  1.193, 1.248, 1.413,  # sigma
  0.068, 0.128, 0.340  # rho
)
@
%
The final model specification reads
%
<<formula-xinyi>>=
f <-
  twing_status | vmd_ln ~
  edu_2 + edu_3 + hhincome_2 + hhincome_3 + flex_work + work_fulltime +
  twing_feasibility + att_proactivemode + att_procarowning + att_wif +
  att_proteamwork + att_tw_effective_teamwork + att_tw_enthusiasm +
  att_tw_location_flex |
  female + age_mean + age_mean_sq + race_black + race_other + vehicle +
  suburban + smalltown + rural + work_fulltime + att_prolargehouse +
  att_procarowning + region_waa |
  edu_2 + edu_3 + suburban + smalltown + rural + work_fulltime +
  att_prolargehouse + att_proactivemode + att_procarowning |
  female + hhincome_2 + hhincome_3 + child + suburban + smalltown +
  rural + att_procarowning + region_waa
@
%
and the model can be estimated by
%
<<model-xinyi>>=
start_default <- opsr(f, telework_data, .get2step = TRUE)
fit <- opsr(f, telework_data, start = start, method = "NM", iterlim = 50e3,
  printLevel = 0)
@
%
where we demonstrate that
\begin{enumerate}
\item Default starting values as computed by the Heckman two-step procedure can be retrieved.
\item \code{start} values can be overridden (we have hidden the \code{start} vector here for brevity). If the user wishes to pass start values manually, some minimal conventions have to be followed as documented in \code{?opsr_check_start}.
\item Alternative maximization methods (here ``Nelder-Mead'') can be used (as in the original paper).
\end{enumerate}
%
<<hidden, echo=FALSE>>=
custom.model.names <- c("Selection", "NTWer (535)", "NUTWer (322)", "UTWer (727)")
custom.coef.names <- c(
  "Some college",
  "Bachelor's degree or higher",
  "\\$50,000 to \\$99,999",
  "\\$100,000 or more",
  "Flexible work schedule",
  "Full time worker",
  "Teleworking feasibility",
  "Pro-active-mode",
  "Pro-car-owning",
  "Work interferes with family",
  "Pro-teamwork",
  "TW effective teamwork",
  "TW enthusiasm",
  "TW location flexibility",
  "Intercept",
  "Female",
  "Age",
  "Age squared",
  "Black",
  "Other races",
  "Number of vehicles",
  "Suburban",
  "Small town",
  "Rural",
  "Pro-large-house",
  "Region indicator (WAA)",
  "Number of children"
)
reorder.coef <- c(1:14, 25, 15:24, 26, 27)
groups <- list(
  "Education (ref: high school or less)" = 1:2,
  "Household income (ref: less than \\$50,000)" = 3:4,
  "Attitudes" = 8:15,
  "Race (ref: white)" = 20:21,
  "Residential location (ref: urban)" = 23:25
)
@
%
With help of the \pkg{texreg} package, production-grade tables (in various output formats) can be generated with ease.
%
<<replica, results=tex>>=
texreg::texreg(
  fit, beside = TRUE, include.structural = FALSE, include.R2 = TRUE,
  include.pseudoR2 = TRUE, custom.model.names = custom.model.names,
  custom.coef.names = custom.coef.names, reorder.coef = reorder.coef,
  groups = groups, scalebox = 0.83, booktabs = TRUE, dcolumn = TRUE,
  use.packages = FALSE, float.pos = "t!", single.row = TRUE,
  caption = "Replica of \\cite{Wang+Mokhtarian:2024}, Table 3.",
  label = "tab:wang-replica"
)
@
%
Dot arguments (\code{...}) passed to \fct{texreg} (or similar functions) are forwarded to a \proglang{S4} method \fct{extract} which extracts the variables of interest from a model fit (see also \code{?extract.opsr}). We demonstrate here that
\begin{enumerate}
\item Model components can be omitted. Here, the structural coefficients (\code{kappa}, \code{sigma}, \code{rho}) are disgarded (\code{include.structural = FALSE}).
\item The model components can be printed side-by-side (\code{beside = TRUE}).
\item Additional goodness-of-fit indicators can be included (\code{include.R2 = TRUE} and \code{include.pseudoR2 = TRUE}). Note that the indicators are repeated for all the model components.
\item The output formatting can be controlled flexibly, by reordering, renaming and grouping coefficients (the fiddly but trivial details are hidden here for brevity).
\end{enumerate}


%% -- Case study ---------------------------------------------------------------
\section{Case study} \label{sec:case-study}

Now, that the reader is familiar with the main functionality of \pkg{OPSR}, this section demonstrates how to employ it in a real-world example. The emphasis, therefore, lies not on what each function does but on guiding the reader through the modeling and post-estimation steps. We investigate once again, telework treatment effects on weekly distance traveled (aggregated over all modes of transport).

We first discuss the model building strategy to arrive at an appropriately specified OPSR model. We then demonstrate, why error correlation occurs, having omitted a variable simultaneously influencing the selection and outcome process. The OPSR models are compared to models not accounting for this error correlation and implications for treatment effects are shown. The case study concludes with a discussion on unit treatment effects investigating to what degree foregone commutes (when teleworking) are compensated with leisure travel.

%% The data
We use the TimeUse+ dataset \citep{Winkler+Meister+Axhausen:2024}, a smartphone-based diary, recording travel, time use, and expenditure data. Our analytical sample comprises employed individuals and is based on what \citet{Winkler+Axhausen:2024} identified as valid days. A valid day has at least 20 hours of information where 70\% of the events were validated by the user. Users who did not have at least 14 valid days were excluded. For the remaining 824 participants mobility indicators for a typical week were constructed. The telework status is based on tracked (and labelled) work activities and three regimes are differentiated: Non-teleworkers (NTW), Non-usual teleworkers (NUTW; $<$3 days/week) and Usual teleworkers (UTW; 3$+$ days/week).

The data, underlying this analysis, is attached, documented (\code{?timeuse_data}) and can be loaded by
%
<<timeuse-data>>=
data("timeuse_data", package = "OPSR")
@
%
A basic boxplot of the response variable against the three telework statuses is displayed in Figure~\ref{fig:boxplot}. By simply looking at the data descriptively, we might prematurely conclude that telework does not impact weekly distance traveled. However, the whole value proposition of OPSR (and of models in general) is that we really are interested in a counterfactual. If the teleworkers self-select, the counterfactual is not simply the group average. More prosaically, if the usual telewokers (UTW) would choose to be non-teleworkers (NTW), they might travel more or less than the actual NTWers.

Meanwhile, commute distance increases across the three teleworker groups, suggesting that, one, longer commutes increase the propensity to telework and two, teleworkers have a higher share of leisure travel (given the similar overall distance traveled).

\setkeys{Gin}{width=.8\textwidth}
\begin{figure}[t!]
\centering
<<boxplot, echo=FALSE, fig=TRUE, height=5, width=9>>=
par(mfrow = c(1, 2))
plot(log_weekly_km ~ factor(wfh), data = timeuse_data, varwidth = TRUE,
  ylab = "Log weekly distance traveled (km)", xlab = "Telework status",
  names = c("NTW", "NUTW", "UTW"), main = "Weekly distance traveled",
  col = "white")
plot(log_commute_km ~ factor(wfh), data = timeuse_data, varwidth = TRUE,
  ylab = "Log commute distance (km)", xlab = "Telework status",
  names = c("NTW", "NUTW", "UTW"), main = "Commute distance",
  col = "white")
@
\caption{\label{fig:boxplot} Log weekly distance traveled and log commute distance for different telework statuses.}
\end{figure}

%% The model
Before blindly trying to specify a full model using \pkg{OPSR} the analyst is advised to first, think of an identification restriction as mentioned in Section~\ref{sec:model} and second, estimate the models separately, e.g., using \fct{polr} from the \pkg{MASS} package \citep{Venables+Ripley:2002}, and \fct{lm} for the treatment groups separately. We reserve the international standard classification of occupations (ISCO-08) variables for the selection process.
%
<<fit-polr>>=
drop <- c("id", "weekly_km", "log_weekly_km", "commute_km", "log_commute_km",
  "wfh_days")
dat_polr <- subset(timeuse_data, select = !(names(timeuse_data) %in% drop))
dat_polr$wfh <- factor(dat_polr$wfh)
fit_polr <- MASS::polr(wfh ~ ., dat_polr, method = "probit")
@
%
The \fct{stepAIC} function chooses a selection model specification by AIC in a stepwise algorithm.
%
<<step>>=
fit_step <- MASS::stepAIC(fit_polr, trace = FALSE)
fit_step$anova
@
%
Fitting the linear models separately, benefits an understanding of potential identification problems (e.g., colinear variables or missing factor levels in one of the groups). While the resulting estimates are potentially biased and their standard errors not reliable, it can still help to have a closer look at resulting estimates and goodness of fit indicators.
%
<<fit-outcome>>=
fit_lm <- function(data, group) {
  f <- paste0("log_weekly_km ~ . - wfh")
  dat <- subset(data, subset = wfh == group)
  fit <- lm(f, dat)
  fit
}

drop <- c("id", "weekly_km", "commute_km", "log_commute_km", "wfh_days")
dat_lm <- subset(timeuse_data,
  select = !(names(timeuse_data) %in% drop) & !grepl("^isco_", names(timeuse_data)))

fit_ntw <- fit_lm(dat_lm, group = 1)
fit_nutw <- fit_lm(dat_lm, group = 2)
fit_utw <- fit_lm(dat_lm, group = 3)
summary(fit_utw)
@
%
Here, we have two singularity issues for the UTWers: First, \code{shift_work} is a constant and second, \code{parking_home} is colinear with \code{car_access}.

We then follow the conventional (somewhat heuristic) model building strategy to specify the full identified model and then exclude all variables that do not produce significant estimates (at the 10\% level). The formula specification of the full model is hidden here for brevity.
%
<<hidden, echo=FALSE>>=
f_full <- wfh | log_weekly_km ~
  age + educ_higher + hh_income + young_kids + workload + fixed_workplace +
  shift_work + permanent_employed + isco_craft + isco_tech + isco_clerical +
  isco_elementary + car_access + parking_home + freq_onl_order +
  grocery_shopper |
  sex_male + age + educ_higher + swiss + married + res_loc + dogs + hh_size +
  young_kids + n_children + workload + fixed_workplace + permanent_employed +
  driverlicense + car_access + parking_home + parking_work + rents_home +
  freq_onl_order + vacation + grocery_shopper |
  sex_male + age + educ_higher + swiss + married + res_loc + dogs + hh_size +
  young_kids + n_children + workload + fixed_workplace + permanent_employed +
  driverlicense + car_access + parking_home + parking_work + rents_home +
  freq_onl_order + vacation + grocery_shopper |
  sex_male + age + educ_higher + swiss + married + res_loc + dogs + hh_size +
  young_kids + n_children + workload + fixed_workplace + permanent_employed +
  driverlicense + car_access + parking_work + rents_home +
  freq_onl_order + vacation + grocery_shopper
@
%
<<full-reduced>>=
fit_full <- opsr(f_full, timeuse_data, printLevel = 0)
f_red <- wfh | log_weekly_km ~
  age + educ_higher + hh_income + young_kids + workload + fixed_workplace +
  shift_work + permanent_employed + isco_craft + isco_tech + isco_clerical +
  isco_elementary + car_access + parking_home + freq_onl_order +
  grocery_shopper |
  sex_male + res_loc + workload + permanent_employed + parking_work |
  swiss + res_loc + young_kids + workload + parking_work |
  sex_male + swiss + fixed_workplace + permanent_employed + parking_work

fit_red <- opsr(f_red, timeuse_data, printLevel = 0)
print(anova(fit_red, fit_full), print.formula = FALSE)
summary(fit_red)
@
%
The reduced model specification (\code{fit_red}) is not rejected in the likelihood ratio test. Further, there is significant error correlation between the selection process and the outcome process for the UTWers (\code{rho3}). The Wald-test suggests that the null hypothesis (\code{rho1} = \code{rho2} = \code{rho3} = 0) can be rejected at the 5\% level, suggesting that OPSR is beneficial given our model assumptions.

However, so far we have neglected the commute distance which most likely impacts the propensity to telework (see Figure~\ref{fig:boxplot}) and naturally influences the weekly distance traveled. To illustrate this, the reduced model specification can be updated to include \code{log_weekly_km}
%
<<adding-commute>>=
fit_commute <- update(fit_red, ~ . + log_commute_km | . + log_commute_km | . +
  log_commute_km | . + log_commute_km)

print(summary(fit_commute), print.call = FALSE)
@
%
where now all goodness of fit indicators improved (in particular R$^2$ for the continuous outcomes) and the rho coefficients are no longer significant at conventional levels. Similarly, the Wald-test (rho) can no longer reject the null at the 10\% level. Meanwhile, some of the coefficients slightly changed in magnitude and rendered insignificant or vice versa. For example, the effect of residential location (\code{o1_res_loc_rural} and \code{o2_res_loc_rural}) moderated the effect of commute distance in \code{fit_red}, suggesting that individuals living in more rural locations tend to have longer commutes.

While this discussion (of omitted variable bias and/or endogeneity) is common for all regression analysis, it highlights here, why error correlation can occur. Both model specifications (\code{fit_red} and \code{fit_commute}) produce similar insights in the post-estimation that follows. However, we will demonstrate later, that not accounting for error correlation can lead to reverse (and most likely false) conclusions.

%% The treatment effects
We first define some helper functions to compute treatment effects.
%
<<utils>>=
tw_status <- c("NTW", "NUTW", "UTW")

estimated_weekly_km <- function(object, type = "unlog-response") {
  nReg <- object$nReg
  out <- vector("list", nReg)
  counterfacts <- vector("list", nReg)
  for (g in 1:nReg) {
    for (c in 1:nReg) {
      counterfacts[[c]] <- predict(object, group = g, counterfact = c, type = type)
    }
    df <- as.data.frame(counterfacts)
    names(df) <- tw_status
    out[[g]] <- df
  }
  names(out) <- tw_status
  out
}

average <- function(object) {
  ae <- lapply(object, function(x) {
    apply(x, 2, function(x) mean(x, na.rm = TRUE))
  })
  as.data.frame(ae)
}

pairwise_diff <- function(mat) {
  n <- nrow(mat)
  m <- ncol(mat)
  result <- matrix(NA, nrow = n, ncol = m)
  for (j in 1:m) {
    result[, j] <- c(
      mat[2, j] - mat[1, j],
      mat[3, j] - mat[1, j],
      mat[3, j] - mat[2, j]
    )
  }
  rownames(result) <- c("NTW -> NUTW", "NTW -> UTW", "NUTW -> UTW")
  colnames(result) <- c("NTW", "NUTW", "UTW")
  result
}

ate <- function(object) {
  awk <- average(estimated_weekly_km(object))
  ate <- pairwise_diff(awk)
  ate
}

ate(fit_commute)
@
%
Telework reduces weekly kilometers traveled across all groups with the exception of NTWers who would slightly be more mobile when switching from NTW to NUTW (\code{NTW -> NUTW}). The treatment effects when switching from NTW to NUTW are strongest for UTWers, who have generally longer commutes. Treatment effects for NTW to UTW are similar across all three groups, again slightly stronger for UTWers. Interestingly, NTWers show a non-linear pattern, first increasing weekly kilometers when adopting some telework (NTW to NUTW) but then substantially decreasing weekly kilometers with more telework (NUTW to UTW). An explanation could be, that these individuals (living closer to their workplace) do initially not adjust activity chains and location choices when only occasionally teleworking. For example, an individual might stay subscribed to the gym close to the workplace and visit that facility even on a home office day. On the other hand, UTWers show somewhat an inverse pattern, first (NTW to NUTW) strongly reducing weekly kilometers but upon further telework adoption (NUTW to UTW) only minimally adjusting weekly kilometers. A similar argument could be made, that these individuals (living further from their workplace) already from the start adjust activity chains and location choices. One can therefore conclude, that the treatment effect over the full range (NTW to UTW) is similar across all groups but the main travel reduction happens at different treatment intensities. Figure~\ref{fig:treatment} (panel d) visualizes these average treatment effects and shows the linear pattern for NUTW and the (mirrored) hockey stick pattern for NTW and UTW.

While the discussion above was based on average treatment effects, Figure~\ref{fig:treatment} shows the distributions of predicted weekly distance traveled by teleworker group. Each panel presents a pair of (un)treated telework statuses as the margins and the dashed lines are the empirical sample means. The solid black reference line marks the instances where weekly distance traveled is equal for both of the paired (un)treated telework statuses. I.e., points below the reference line indicate more travel under the regime depicted on the x-axis.
%
<<hidden, echo=FALSE>>=
plot_treat_avg <- function(object, x, ylim, col, main,
                           legend.position = "bottomright",
                           pch = par("pch"), lwd, ...) {

  op <- par(no.readonly = TRUE)
  on.exit(par(op))

  tw_status <- c("NTW", "NUTW", "UTW")
  xlabs <- paste0(tw_status, "\n(", round(x, 2), " d/week)")
  awk <- average(estimated_weekly_km(object))
  input <- apply(awk, 2, function(x) {
    dat <- data.frame(y = x)
    dat$x <- 1:nrow(dat)
    dat
  })

  par(...)
  plot(x, input[[1]]$y, type = "n",
       ylim = ylim,
       xlab = "Telework treatment", ylab = "Weekly distance (km)",
       xaxt = "n")
  mtext(main, line = 2, font = 2, cex = 1)
  grid()
  axis(1, at = x, labels = xlabs, padj = 0.5)
  for (i in seq_along(input)) {
    lines(x = x, y = input[[i]]$y, type = "b", col = col[i], lwd = lwd)
  }
  legend(legend.position, legend = tw_status, pch = 1, lwd = lwd, col = col, bty = "n")

  invisible()
}

plot_treat_obs <- function(object, x, y, type, xlim = NULL, ylim = NULL, main = NULL,
                           xlab = NULL, ylab = NULL, col, alpha = 1, lines.fun = mean,
                           lwd = par("lwd"), lty = par("lty"), pch = par("pch"),
                           cex.point = 1, cex.text = 1, mar = c(5.1, 4.1, 0, 0),
                           oma = c(0, 0, 3, 0), legend.position = "topleft",
                           legend.entry = 1:object$nReg, digits = 0, ...) {

  op <- par(no.readonly = TRUE)
  on.exit(par(op))

  ## treatment
  ewk <- estimated_weekly_km(object, type = type)
  px <- lapply(ewk, function(g) g[[x]])
  py <- lapply(ewk, function(g) g[[y]])

  ## limits
  limits <- function(f, x) f(Reduce(c, x), na.rm = TRUE)
  minx <- limits(min, px)
  maxx <- limits(max, px)
  miny <- limits(min, py)
  maxy <- limits(max, py)
  mpx <- lapply(px, function(x) lines.fun(x, na.rm = TRUE))
  mpy <- lapply(py, function(x) lines.fun(x, na.rm = TRUE))
  if (is.null(xlim)) xlim <- c(minx, maxx)
  if (is.null(ylim)) ylim <- c(miny, maxy)

  ## plot functions
  base_plot <- function() {
    plot(1, 1, type = "n", xlim = xlim, ylim = ylim, xlab = xlab, ylab = ylab,
         bty = "o")
    # grid()
    for (i in seq_len(object$nReg)) {
      points(px[[i]], py[[i]], col = scales::alpha(col[i], alpha), pch = pch,
             cex = cex.point)
      abline(v = mpx[[i]], col = col[i], lty = lty, lwd = lwd)
      text(x = mpx[[i]], y = ylim[1] + 10, labels = round(mpx[[i]], digits),
           col = col[i], adj = c(-0.1, 1), cex = cex.text)
      abline(h = mpy[[i]], col = col[i], lty = lty, lwd = lwd)
      text(x = xlim[1] + 10, y = mpy[[i]], labels = round(mpy[[i]], digits),
           col = col[i], adj = c(0.3, -0.1), cex = cex.text)
    }
    abline(a = 0, b = 1, lty = 1, lwd = lwd)
    legend(legend.position, legend = legend.entry, col = col, pch = pch, bty = "n")
  }

  x_density <- function(px) {
    dx <- lapply(px, function(x) density(x, na.rm = TRUE))
    maxy <- which.max(sapply(dx, function(x) max(x$y)))
    plot(dx[[maxy]], type = "n", xlim = xlim, axes = FALSE, ann = FALSE,
         zero.line = FALSE)
    for (i in seq_along(dx)) {
      polygon(dx[[i]], col = scales::alpha(col[i], 0.3), border = col[i], lty = 1)
    }
  }

  y_density <- function(py) {
    dy <- lapply(py, function(x) {
      dy <- density(x, na.rm = TRUE)
      x <- dy$x
      dy$x <- dy$y
      dy$y <- x
      dy
    })
    maxx <- which.max(sapply(dy, function(x) max(x$x)))
    plot(dy[[maxx]], type = "n", ylim = ylim, axes = FALSE, ann = FALSE,
         zero.line = FALSE)
    for (i in seq_along(dy)) {
      polygon(dy[[i]], col = scales::alpha(col[i], 0.3), border = col[i], lty = 1)
    }
  }

  ## build plot
  if (!is.null(title)) {
    par(oma = oma)
  }
  mat <- matrix(c(2, 0, 1, 3), nrow = 2, ncol = 2, byrow = TRUE)
  layout(mat = mat, heights = c(1, 6), widths = c(6, 1), respect = TRUE)
  par(mar = mar)
  base_plot()
  par(mar = c(0, mar[2], 0, 0))
  x_density(px)
  ## add title
  if (!is.null(main)) {
    mtext(main, line = 1, font = 2)
  }
  par(mar = c(mar[1], 0, 0, 0))
  y_density(py)

  invisible()
}
@
%
\setkeys{Gin}{width=\textwidth}
\begin{figure}[t!]
\centering
<<plot-treatment, echo=FALSE, fig=TRUE, height=8, width=8>>=
x <- aggregate(wfh_days ~ wfh, data = timeuse_data, FUN = mean)$wfh_days

fit <- fit_commute

colvec <- c("#023047", "#fb8500", "#058c42")
colvec <- RColorBrewer::brewer.pal(n = 3, "Dark2")
pch <- 19
lty <- 3
alpha <- 0.3
lwd <- 3

g4 <- function() {
  plot_treat_avg(fit, x, ylim = c(0, 300), col = colvec,
                 main = "d) Average treatment effect",
                 cex = 0.8, lwd = lwd)
}
g4grob <- gridGraphics::echoGrob(g4)

g1 <- function() {
  plot_treat_obs(fit, x = 1, y = 2, type = "unlog-response",
                 main = "a) Not TWing vs. TWing < 3 times/week",
                 xlab = "Weekly distance when NTW (km)",
                 ylab = "Weekly distance when NUTW (km)", col = colvec, alpha = alpha,
                 cex.point = 1.5, pch = pch, lty = lty, lwd = lwd,
                 legend.entry = c("NTW", "NUTW", "UTW"),
                 legend.position = "bottomright")
}
g1grob <- gridGraphics::echoGrob(g1)

g2 <- function() {
  plot_treat_obs(fit, x = 1, y = 3, type = "unlog-response",
                 main = "b) Not TWing vs. TWing 3+ times/week",
                 xlab = "Weekly distance when NTW (km)",
                 ylab = "Weekly distance when UTW (km)", col = colvec, alpha = alpha,
                 cex.point = 1.5, pch = pch, lty = lty, lwd = lwd,
                 legend.entry = c("NTW", "NUTW", "UTW"),
                 legend.position = "bottomright")
}
g2grob <- gridGraphics::echoGrob(g2)

g3 <- function() {
  plot_treat_obs(fit, x = 2, y = 3, type = "unlog-response",
                 main = "c) TWing < 3 times/week vs. TWing 3+ times/week",
                 xlab = "Weekly distance when NUTW (km)",
                 ylab = "Weekly distance when UTW (km)", col = colvec, alpha = alpha,
                 cex.point = 1.5, pch = pch, lty = lty, lwd = lwd,
                 legend.entry = c("NTW", "NUTW", "UTW"),
                 legend.position = "bottomright")
}
g3grob <- gridGraphics::echoGrob(g3)

gridExtra::grid.arrange(g1grob, g2grob, g3grob, g4grob, ncol = 2)
@
\caption{\label{fig:treatment} Treatment effects.}
\end{figure}

As already alluded, not controlling for commute distance implies that selection on unobservables exists, leading to error correlation and selection bias if not accounted for. As we will illustrate now, this also compromises treatment effects. Recalling that \code{fit_red} is our final model without commute distance (but significant error correlation, as we have seen), we derive a model (\code{fit_nocor}) without error correlation by setting the \code{rho} coefficients to 0. I.e., this is the same as separately estimating an ordered probit model and three linear regression models.
%
<<no-cor>>=
start <- coef(fit_red)
fixed <- c("rho1", "rho2", "rho3")
start[fixed] <- 0
fit_nocor <- opsr(f_red, timeuse_data, start = start, fixed = fixed,
  printLevel = 0)
@
%
The average treatment effects are
%
<<>>=
ate(fit_red)
ate(fit_nocor)
@
%
While resulting treatment effects based on \code{fit_red} are comparable to the ones based on \code{fit_commute}, \code{fit_nocor} yields completely different insights, in particular, that telework generally increases weekly distance traveled.

Recall that for \code{fit_commute}, the Wald-test (rho) could not reject the null ($p$~value \Sexpr{round(summary(fit_commute)$wald$rho$pval, 2)}). Therefore, adding \code{log_commute_km} to the model without error correlation (\code{fit_nocor}) might yield less biased treatment effects
%
<<no-cor-commute>>=
start <- coef(fit_commute)
start[fixed] <- 0
fit_nocor2 <- opsr(fit_commute$formula, timeuse_data, start = start,
  fixed = fixed, printLevel = 0)
ate(fit_nocor2)
@
%
where now the direction of the treatment effects aligns with the OPSR models but the values are still considerably different. Since \code{fit_nocor2} is a nested model of \code{fit_commute} we can conduct a likelihood ratio test
%
<<no-cor-test>>=
print(anova(fit_nocor2, fit_commute), print.formula = FALSE)
@
%
which does not reject the null (at the 10\% level) that the simpler model is sufficient. As a conclusion should be noted that the modeled covariance matrix (in particular the magnitude of \code{rho}) potentially strongly influences the treatment effects.

Lastly (using \code{fit_commute}), we would like to investigate to what degree foregone commute distance (when teleworking) is compensated with leisure travel. Therefore, we compute unit treatment effects and compare them to the average two-way commute distance for each group. The unit treatment effect is calculated by dividing the total treatment effect by the corresponding average teleworking frequency difference (\code{twdiff1} to \code{twdiff3} below). I.e., the treatment effect is standardized and therefore also comparable for different regime switching (e.g., NTW to NUTW vs. NUTW to UTW).
%
<<unit-treatment-effects>>=
dat_ute <- subset(timeuse_data, select = c(commute_km, wfh, wfh_days))
dat_ute <- aggregate(cbind(wfh_days, 2 * commute_km) ~ wfh, data = dat_ute,
  FUN = mean)

top <- t(dat_ute[2:3])
colnames(top) <- c("NTW", "NUTW", "UTW")
rownames(top) <- c("WFH (days)", "2-way commute (km)")

i <- "WFH (days)"
twdiff1 <- top[i, "NUTW"] - top[i, "NTW"]
twdiff2 <- top[i, "UTW"] - top[i, "NTW"]
twdiff3 <- top[i, "UTW"] - top[i, "NUTW"]

twdiff <- matrix(c(rep(twdiff1, 3), rep(twdiff2, 3), rep(twdiff3, 3)), nrow = 3)
bottom <- ate(fit_commute) / twdiff

ute <- rbind(top, bottom)
ute
@
%
Generally, telework reduces weekly distance traveled by less than the foregone commute distance, which indicates, that a rebound effect (compensating leisure travel) exists. For example, the NUTWers could save 43.33 km in commute travel but only reduce 5.2 km per marginal teleworking day when switching from NTW to NUTW. This compensating travel exists for all TW groups except the NTWers (NTW to UTW and NUTW to UTW), where we observe diminished travel activity beyond foregone commutes. The insights from the discussion on average treatment effects caries over: Adjustments in weekly distance traveled are very different both across the three teleworker groups but also across the regime switching.

%% -- Summary/conclusions/discussion -------------------------------------------

\section{Summary and discussion} \label{sec:summary}

In a real-world setting, the treatment is usually not exogenously prescribed but self-selected. Various methods in various statistical environments exist to account for selection-bias which arises if unobserved factors simultaneously influence both the selection and outcome process. OPSR is introduced as a special case of endogenous switching regression. The model frame for such Heckman-type models as well as their implementation in the \proglang{R} system for statistical computing is reviewed. The here presented \proglang{R} implementation in package \pkg{OPSR} re-uses design and functionality of the corresponding \proglang{R} software. Hence, the new function \fct{opsr} is straightforward to apply for model fitting and diagnostics. Further, it is fast and memory efficient thanks to the \proglang{C++} implementation of the log-likelihood function which can also be parallelized. \pkg{OPSR} handles log-transformed outcomes which need special consideration when computing conditional expectations and thus treatment effects. In the case study, the OPSR method is applied to a tracking and activity diary dataset, investigating the telework treatment effects on weekly distance traveled. We demonstrate, first, why error correlation occurs and second, in how far computed treatment effects differ if the error correlation is not accounted for. We find that non-teleworkers tend to have shorter commutes and adjust mobility patterns mainly when switching from non-usual telework to usual telework. On the other hand, weekly distance traveled slightly increases when initially adopting some telework. Contrary, usual teleworkers (had they not been teleworking) adjust mobility patterns strongly when adopting some telework but then only marginally reduce distance traveled when further adopting telework. Comparing the unit treatment effects to the two-way commute distance indicates that telework generally reduces weekly distance traveled and it does so by less than the foregone commute. Therefore, some compensating travel (rebound effects) exists for most of the teleworker groups.


%% -- Optional special unnumbered sections -------------------------------------

\section*{Computational details}

The results in this paper were obtained using
\proglang{R}~\Sexpr{paste(R.Version()[6:7], collapse = ".")} with the packages
\pkg{OPSR}~\Sexpr{packageVersion("OPSR")}, \pkg{MASS}~\Sexpr{packageVersion("MASS")}, \pkg{texreg}~\Sexpr{packageVersion("texreg")}, \pkg{gridExtra}~\Sexpr{packageVersion("gridExtra")} and \pkg{gridGraphics}~\Sexpr{packageVersion("gridGraphics")}. \proglang{R} itself
and all packages used are available from the Comprehensive
\proglang{R} Archive Network (CRAN) at
\url{https://CRAN.R-project.org/}.


\section*{Acknowledgments}
