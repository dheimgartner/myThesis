\RequirePackage{fix-cm}
\documentclass[%
    twoside, openright, titlepage, numbers=noenddot,%
    cleardoublepage=empty,%
    abstract=false,%
    BCOR=5.5mm, paper=a5, fontsize=10pt,% A5 soft cover
    %BCOR=5.5mm, paper=17cm:24cm, fontsize=10pt,% 17 cm x 24 cm
    %BCOR=5mm, paper=15.59cm:23.39cm, fontsize=10pt,% Royal soft cover
    %BCOR=0mm, paper=15.24cm:22.86cm, fontsize=10pt,% US-Trade hard cover
]{scrreprt}

\input{preamble/general}
\usepackage[final]{pdfpages}

% Custom commands
\input{preamble/commands}

% Custom packages
\input{preable/packages}

% Bibliography
\addbibresource[label=ownpubs]{ownpubs.bib}
\addbibresource{bibliography.bib}
\addbibresource{misc.bib}




\usepackage{Sweave}
\begin{document}
\frenchspacing
\raggedbottom%
\selectlanguage{english}
\pagenumbering{roman}
\pagestyle{scrplain}

%
% Cover
%
% Uncomment and adapt these lines if you want to include a cover PDF.
%
\includepdf[pages={1,{}}]{cover/crop/cover_front.pdf}
\cleardoublepage\setcounter{page}{1}

%
% Frontmatter
%

\include{frontbackmatter/dirtytitlepage}
\include{frontbackmatter/titlepage}
\include{frontbackmatter/titleback}
\cleardoublepage\include{frontbackmatter/dedication}
\cleardoublepage\include{frontbackmatter/abstract}
\cleardoublepage\include{frontbackmatter/acknowledgments}
\pagestyle{scrheadings}
\cleardoublepage\include{frontbackmatter/contents}

%
% Mainmatter
%
\cleardoublepage\pagenumbering{arabic}%
\def\dir{chapters/introduction}
\include{\dir/main}

\cleardoublepage%
\def\dir{chapters/sample-chapter}
\include{\dir/main}

% Include some blind text
\Blinddocument

\cleardoublepage%
\chapter{Sweave Chapter}
\label{ch:sweave-chapter}

\dictum[Daniel Heimgartner]{%
  All models are wrong - some models are wronger than others! }%
\vskip 1em

\begin{Schunk}
\begin{Sinput}
R> head(iris)
\end{Sinput}
\begin{Soutput}
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa
2          4.9         3.0          1.4         0.2  setosa
3          4.7         3.2          1.3         0.2  setosa
4          4.6         3.1          1.5         0.2  setosa
5          5.0         3.6          1.4         0.2  setosa
6          5.4         3.9          1.7         0.4  setosa
\end{Soutput}
\end{Schunk}


\cleardoublepage%
%% -- Introduction -------------------------------------------------------------

%% - In principle "as usual".
%% - But should typically have some discussion of both _software_ and _methods_.
%% - Use \proglang{}, \pkg{}, and \code{} markup throughout the manuscript.
%% - If such markup is in (sub)section titles, a plain text version has to be
%%   added as well.
%% - All software mentioned should be properly \cite-d.
%% - All abbreviations should be introduced.
%% - Unless the expansions of abbreviations are proper names (like "Journal
%%   of Statistical Software" above) they should be in sentence case (like
%%   "generalized linear models" below).

\section{Introduction} \label{sec:intro}

The goal of the program evaluation literature is to estimate the effect of a treatment program (e.g., a new policy, technology, medical treatment, or agricultural practice) on an outcome. To evaluate such a program, the ``treated'' are compared to the ``untreated''. In an experimental setting, the treatment can be (randomly) assigned by the researcher. However, in an observational setting, the treatment is not always exogenously prescribed but rather self-selected. This gives rise to a selection bias when unobserved factors influencing the treatment adoption also influence the outcome (also known as \emph{selection on unobservables}). Simple group comparison no longer yield an unbiased estimate of the treatment effect. In more technical terms, the counterfactual outcome of the treated (``if they had not been treated'') does not necessarily correspond to the factual outcome of the untreated. For example, cyclists riding without a helmet (the ``untreated'') might have a risk-seeking tendency. We therefore potentially overestimate the benefit of wearing a helmet if we compare the accident (severity) rate of the two groups. Risk-seeking is not readily measured and it is easy to imagine that it becomes part of the error in applied research and thus leading cause of a selection bias.

To properly account for the selection bias, various techniques exist, both for longitudinal and cross-sectional data. In the first case, difference in differences is a widely adopted measure. In the latter case, instrumental variables, matching propensity scores, regression-discontinuity design, and the endogenous switching regression model have been applied \citep{Wang+Mokhtarian:2024}. The latter method is particularly well-suited to correct for both selection on observables and unobservables (unlike other methods which only address and correct for selection on observables).

The seminal work by \cite{Heckman:1979} proposed a two-part model to address the selection bias that often occurs when modelling a continuous outcome which is only observable for a subpopulation. A very nice exposition of this model is given in \citet[][Chapter~16]{Cameron+Trivedi:2005}. The classical Heckman model consists of a probit equation and continuous outcome equation. A natural extension is then switching regression, where the population is partitioned into different groups (regimes) and separate parameters are estimated for the continuous outcome process of each group. This model is originally known as the Roy model \citep{Cameron+Trivedi:2005} or Tobit 5 model \citep{Amemiya:1985}. These classical models (the Tobit models for truncated, censored or interval data and their extensions) are implemented in various environments for statistical computing and in \proglang{R}'s \citep{R} \pkg{sampleSelection} package \citep{Toomet+Henningsen:2008}.

Many different variants can then be derived by either placing different distributional assumptions on the errors and/or how the latent process manifests into observed outcomes (i.e., the dependent variables can be of various types, such as binary, ordinal, censored, or continuous) more generally known as conditional mixed-process (CMP) models. CMP models comprise a broad family involving two or more equations featuring a joint error distribution assumed to be multivariate normal. The \proglang{Stata} \citep{Stata} command \code{cmp} \citep{Roodman:2011} can fit such models. The variant at the heart of this paper is an ordered probit switching regression (OPSR) model, with ordered treatments and continuous outcome. Throughout the text we use the convention that OPSR refers to the general methodology, while \pkg{OPSR} refers specifically to the package.

OPSR is available as a \proglang{Stata} command, \code{oheckman} \citep{Chiburis+Lokshin:2007}, which however, does not allow distinct specifications for the continuous outcome processes (i.e., the same explanatory variables must be used for all treatment groups). The relatively new \proglang{R} package \pkg{switchSelection} \citep{Potanin:2024} allows to estimate multivariate and multinomial sample selection and endogenous switching models with multiple outcomes. These models are systems of ordinal, continuous and multinomial equations and thus nest OPSR as a special case.

\pkg{OPSR} is tailored to one particular method, easy to use (understand, extend and maintain), fast and memory efficient. Unlike the mentioned implementations, it handles log-transformed continuous outcomes which need special consideration for the computation of conditional expectations. It obeys to \proglang{R}'s implicit modeling conventions (by extending the established generics such as \fct{summary}, \fct{predict}, \fct{update}, \fct{anova} among others) and produces production-grade output tables. This work generalizes the learnings from \cite{Wang+Mokhtarian:2024} and makes the OPSR methodology readily available. The mathematical notation presented here translates to code almost verbatim which hopefully serves a pedagogical purpose for the curious reader.

The remainder of this paper is organized as follows: Section~\ref{sec:model} outlines the ordered probit switching regression model, lists all the key formulas underlying the software implementation and details \pkg{OPSR}'s architecture. In Section~\ref{sec:illustrations} the key functionality is demonstrated both on simulated data and the data from \cite{Wang+Mokhtarian:2024} which we use to reproduce their core model. The case study in Section~\ref{sec:case-study} leverages tracking data from the TimeUse+ study \citep{Winkler+Meister+Axhausen:2024} investigating telework treatment effects on weekly distance traveled. There, we also compare the OPSR models to the ones not accounting for error correlation and discuss the implications for treatment effects. The summary in Section~\ref{sec:summary} concludes.


%% -- Manuscript ---------------------------------------------------------------

%% - In principle "as usual" again.
%% - When using equations (e.g., {equation}, {eqnarray}, {align}, etc.
%%   avoid empty lines before and after the equation (which would signal a new
%%   paragraph.
%% - When describing longer chunks of code that are _not_ meant for execution
%%   (e.g., a function synopsis or list of arguments), the environment {Code}
%%   is recommended. Alternatively, a plain {verbatim} can also be used.
%%   (For executed code see the next section.)

\section{Model and software} \label{sec:model}

In the following, we outline the ordered probit switching regression model as well as list all the key formulas underlying the software implementation. \pkg{OPSR} follows the \proglang{R}-typical formula interface to a workhorse fitter function. Its architecture is detailed after the mathematical part.

As alluded, OPSR is a two-step model: One process governs the ordinal outcome and separate processes (for each ordinal outcome) govern the continuous outcomes. The ordinal outcome can also be thought of as a regime or treatment. In the subsequent exposition, we will refer to the two processes as \emph{selection} and \emph{outcome} process.

We borrow the notation from \cite{Wang+Mokhtarian:2024} where also all the derivations are detailed. For a similar exhibition, \citet{Chiburis+Lokshin:2007} can be consulted. Individual subscripts are suppressed throughout, for simplicity.

Let $\mathcal{Z}$ be a latent propensity governing the selection outcome
%
\begin{equation} \label{eq:selection}
\mathcal{Z} = \Wg + \epsilon,
\end{equation}
%
where $\boldsymbol{W}$ represents the vector of attributes of an individual, $\boldsymbol{\gamma}$ is the corresponding vector of parameters and $\epsilon \sim \mathcal{N}(0, 1)$ a normally distributed error term.

As $\mathcal{Z}$ increases and passes some unknown but estimable thresholds, we move up from one ordinal treatment to the next higher level
%
\begin{equation} \label{eq:thresholds}
Z = j \quad \mathrm{if}\ \kappa_{j-1} < \mathcal{Z} \le \kappa_j,
\end{equation}
%
where $Z$ is the observed ordinal selection variable, $j = 1, \dots, J$ indexes the ordinal levels of $Z$, and $\kappa_j$ are the thresholds (with $\kappa_0 = -\infty$ and $\kappa_J = \infty$). Hence, there are $J-1$ thresholds to be estimated. The probability that an individual self-selects into treatment group $j$ is
%
\begin{equation} \label{eq:prob-selection}
\begin{aligned}
\Prob[Z = j] &= \Prob[\kappa_{j-1} < \mathcal{Z} \le \kappa_j] \\
&= \Prob[\kappa_{j-1} - \Wg < \epsilon \le \kappa_j - \Wg] \\
&= \Phi(\kappa_j - \Wg) - \Phi(\kappa_{j-1} - \Wg).
\end{aligned}
\end{equation}
%
where $\Phi(\cdot)$ is the cumulative distribution function of the standard normal distribution.

The outcome model for the \jth treatment group is expressed as
%
\begin{equation} \label{eq:outcome}
y_j = \Xb + \eta_j,
\end{equation}
%
where $y_j$ is the observed continuous outcome, $\boldsymbol{X_j}$ the vector of observed explanatory variables associated with the \jth outcome model, $\boldsymbol{\beta_j}$ is the vector of associated parameters, and $\eta_j \sim \mathcal{N}(0, \sigma_j^2)$ is a normally distributed error term. At this point it should be noted that $\boldsymbol{X_j}$ and $\boldsymbol{W}$ may share some explanatory variables but not all, due to identification problems otherwise \citep{Chiburis+Lokshin:2007}.

The key assumption of OPSR is now that the errors of the selection and outcome models are jointly multivariate normally distributed
%
\begin{equation} \label{eq:multi-norm}
\begin{pmatrix}
\epsilon \\
\eta_1 \\
\vdots \\
\eta_j \\
\vdots \\
\eta_J
\end{pmatrix}
\sim \mathcal{N}\left(
\begin{pmatrix}
0 \\
0 \\
\vdots \\
0 \\
\vdots \\
0
\end{pmatrix},
\begin{pmatrix}
1 & \rho_1 \sigma_1 & \cdots & \rho_j \sigma_j & \cdots & \rho_J \sigma_J \\
\rho_1 \sigma_1 & \sigma_2^2 \\
\vdots &  & \ddots \\
\rho_j \sigma_j & & & \sigma_j^2 \\
\vdots & & & & \ddots \\
\rho_J \sigma_J & & & & & \sigma_J^2
\end{pmatrix}
\right),
\end{equation}
%
where $\rho_j$ represents the correlation between the errors of the selection model ($\epsilon$) and the \jth outcome model ($\eta_j$). If the covariance matrix should be diagonal (i.e., no error correlation), no selection-bias exists and the selection and outcome models can be estimated separately.

As shown in \cite{Wang+Mokhtarian:2024}, the log-likelihood of observing all individuals self-selecting into treatment $j$ and choosing continuous outcome $y_j$ can be expressed as
%
\begin{multline} \label{eq:log-lik}
\ell(\theta \mid \boldsymbol{W}, \boldsymbol{X_j}) = \sum_{j = 1}^{J} \sum_{\{j\}}
\left\{
\ln\left[
\frac{1}{\sigma_j} \phi\left(\frac{y_j - \Xb}{\sigma_j}\right)
\right] \quad + \right. \\
\left. \ln\left[
\Phi\left(
\frac{\sigma_j (\kappa_j - \Wg) - \rho_j(y_j - \Xb)}{\sigma_j\sqrt{1 - \rho_j^2}}
\right) -
\Phi\left(
\frac{\sigma_j (\kappa_{j-1} - \Wg) - \rho_j(y_j - \Xb)}{\sigma_j\sqrt{1 - \rho_j^2}}
\right)
\right]
\right\}
\end{multline}
%
where $\sum_{\{j\}}$ means the summation of all the cases belonging to the \jth selection outcome, $\phi(\cdot)$ and $\Phi(\cdot)$ are the density and cumulative distribution function of the standard normal distribution.

The conditional expectation can be expressed as
%
\begin{equation} \label{eq:cond-exp}
\begin{aligned}
\E[y_j \mid Z = j] &= \Xb + \E[\eta_j \mid \kappa_{j-1} - \Wg < \epsilon \le \kappa_j - \Wg] \\
&= \Xb - \rho_j\sigma_j \frac{\phi(\kappa_j - \Wg) - \phi(\kappa_{j-1} - \Wg)}{\Phi(\kappa_j - \Wg) - \Phi(\kappa_{j-1} - \Wg)},
\end{aligned}
\end{equation}
%
where the fraction is the ordered probit switching regression model counterpart to the inverse Mills ratio (IMR) term of a binary switching regression model. We immediately see, that regressing $\boldsymbol{X_j}$ on $y_j$ leads to an omitted variable bias if $\rho_j \neq 0$ which is the root cause of the selection bias. However, the IMR can be pre-computed based on an ordered probit model and then included in the second stage regression, which describes the Heckman correction \citep{Heckman:1979}. It should be warned, that since the Heckman two-step procedure includes an estimate in the second step regression, the resulting OLS standard errors and heteroskedasticity-robust standard errors are incorrect \citep{Greene:2002}.

To obtain unbiased treatment effects, we must further evaluate the ``counterfactual outcome'', which reflects the expected outcome under a counterfactual treatment (i.e., for $j' \neq j$)
%
\begin{equation} \label{eq:counterfact-exp}
\begin{aligned}
\E[y_{j'} \mid Z = j] &= \Xbd + \E[\eta_{j'} \mid \kappa_{j-1} - \Wg < \epsilon \le \kappa_j - \Wg] \\
&= \Xbd - \rho_{j'}\sigma_{j'} \frac{\phi(\kappa_j - \Wg) - \phi(\kappa_{j-1} - \Wg)}{\Phi(\kappa_j - \Wg) - \Phi(\kappa_{j-1} - \Wg)}.
\end{aligned}
\end{equation}
%
Let's assume that $y_j = \ln(Y_j + 1)$ in the previous equations. I.e., the continuous outcome was log-transformed as is usual in regression analysis. We have to note, that in such cases the Equations~\ref{eq:cond-exp}-\ref{eq:counterfact-exp} provide the conditional expectation of the log-transformed outcome. Therefore we need to back-transform $Y_j = \exp(y_j) - 1$ which yields
%
\begin{equation} \label{eq:log-cond-exp}
\E[Y_j \mid Z = j] =
\exp\left(\Xb + \frac{\sigma_j^2}{2}\right)
\left[
\frac{\Phi(\kappa_j - \Wg - \rho_j\sigma_j) - \Phi(\kappa_{j-1} - \Wg - \rho_j\sigma_j)}
{\Phi(\kappa_j - \Wg) - \Phi(\kappa_{j-1} - \Wg)}
\right] - 1
\end{equation}
%
for the factual case, and
%
\begin{equation} \label{eq:log-counterfact-exp}
\E[Y_{j'} \mid Z = j] =
\exp\left(\Xbd + \frac{\sigma_{j'}^2}{2}\right)
\left[
\frac{\Phi(\kappa_j - \Wg - \rho_{j'}\sigma_{j'}) - \Phi(\kappa_{j-1} - \Wg - \rho_{j'}\sigma_{j'})}
{\Phi(\kappa_j - \Wg) - \Phi(\kappa_{j-1} - \Wg)}
\right] - 1
\end{equation}
%
for the counterfactual case \citep{Wang+Mokhtarian:2024}.

This concludes the mathematical treatment and we briefly outline \pkg{OPSR}'s architecture which can be conceptualized as follows:
\begin{itemize}
\item We provide the usual formula interface to specify a model. To allow for multiple parts and multiple responses, we rely on the \pkg{Formula} package \citep{Zeileis+Croissant:2010}.
\item After parsing the formula object, checking the user inputs and computing the model matrices, the Heckman two-step estimator is called in \fct{opsr\_2step} to generate reasonable starting values.
\item These are then passed together with the data to the basic computation engine \fct{opsr.fit}. The main estimates are retrieved using maximum likelihood estimation by passing the log-likelihood function \fct{loglik\_cpp} (Equation~\ref{eq:log-lik}) to \fct{maxLik} from the \pkg{maxLik} package \citep{Henningsen+Toomet:2011}.
\item All the above calls are nested in the main interface \fct{opsr} which returns an object of class \class{opsr}. Several methods then exist to post-process this object as illustrated below.
\end{itemize}

The likelihood function \fct{loglik\_cpp} is implemented in \proglang{C++} using \pkg{Rcpp} \citep{Edelbuettel+Balamuta:2018} and relying on the data types provided by \pkg{RcppArmadillo} \citep{Edelbuettel+Sanderson:2014}. Parallelization is available using \proglang{OpenMP}. This makes \pkg{OPSR} both fast and memory efficient (as data matrices are passed by reference).


%% -- Illustrations ------------------------------------------------------------

%% - Virtually all JSS manuscripts list source code along with the generated
%%   output. The style files provide dedicated environments for this.
%% - In R, the environments {Sinput} and {Soutput} - as produced by Sweave() or
%%   or knitr using the render_sweave() hook - are used (without the need to
%%   load Sweave.sty).
%% - Equivalently, {CodeInput} and {CodeOutput} can be used.
%% - The code input should use "the usual" command prompt in the respective
%%   software system.
%% - For R code, the prompt "R> " should be used with "+  " as the
%%   continuation prompt.
%% - Comments within the code chunks should be avoided - these should be made
%%   within the regular LaTeX text.

\section{Illustrations} \label{sec:illustrations}

We first illustrate how to specify a model using \pkg{Formula}'s extended syntax and simulated data. Then the main functionality of the package is demonstrated. We conclude this section by demonstrating some nuances, reproducing the core model of \cite{Wang+Mokhtarian:2024}.

Let us simulate date from an OPSR process with three ordinal outcomes and distinct design matrices $\boldsymbol{W}$ and $\boldsymbol{X}$ (where $\boldsymbol{X} = \boldsymbol{X_j} \ \forall{j}$) by
%
\begin{Schunk}
\begin{Sinput}
R> sim_dat <- opsr_simulate()
R> dat <- sim_dat$data
R> head(dat)
\end{Sinput}
\begin{Soutput}
  ys      yo    xs1    xs2    xo1    xo2
1  1  4.5536 -0.706 -0.435  1.893  1.208
2  3 -4.7574  1.892  0.581 -0.978  1.260
3  2  0.0994 -0.407 -0.513  0.499  0.268
4  2 -1.0102  0.430 -0.648  0.385 -1.201
5  2 -3.0429 -0.648  0.500  0.186 -1.072
6  3 -2.1041  1.197  0.235 -0.611  0.991
\end{Soutput}
\end{Schunk}
%
where \code{ys} is the selection dependent variable (or treatment group), \code{yo} the outcome dependent variable and \code{xs} respectively \code{xo} the corresponding explanatory variables.

Models are specified symbolically. A typical model has the form \code{ys | yo ~ terms_s | terms_o1 | terms_o2 | ...} where the \code{|} separates the two responses and process specifications. If the user wants to specify the same process for all continuous outcomes, two processes are enough (\code{ys | yo ~ terms_s | terms_o}). Hence the minimal \fct{opsr} interface call reads
%
\begin{Schunk}
\begin{Sinput}
R> fit <- opsr(ys | yo ~ xs1 + xs2 | xo1 + xo2, data = dat,
+    printLevel = 0)
\end{Sinput}
\end{Schunk}
%
where \code{printLevel = 0} omits working information during maximum likelihood iterations.

As usual, the fitter function does the bare minimum model estimation while inference is performed in a separate call to
%
\begin{Schunk}
\begin{Sinput}
R> summary(fit)
\end{Sinput}
\begin{Soutput}
Call:
opsr(formula = ys | yo ~ xs1 + xs2 | xo1 + xo2, data = dat, printLevel = 0)

Meta information:
BFGS maximization, 96 iterations
Return code 0: successful convergence 
Runtime: 0.442 secs
Log-Likelihood: -2015 
AIC: 4068 
BIC: 4161 
Number of regimes: 3 
Number of observations: 1000 (161, 497, 342)
Estimated parameters: 19 

Estimates:
               Estimate Std. error t value Pr(> t)    
kappa1          -2.1277     0.1046  -20.35 < 2e-16 ***
kappa2           0.8838     0.0638   13.85 < 2e-16 ***
s_xs1            0.9971     0.0581   17.16 < 2e-16 ***
s_xs2            1.5729     0.0744   21.14 < 2e-16 ***
o1_(Intercept)   1.1067     0.1008   10.98 < 2e-16 ***
o1_xo1           1.9227     0.0723   26.60 < 2e-16 ***
o1_xo2           1.2981     0.0789   16.45 < 2e-16 ***
o2_(Intercept)   1.0817     0.0525   20.62 < 2e-16 ***
o2_xo1          -1.0494     0.0487  -21.54 < 2e-16 ***
o2_xo2           1.4911     0.0527   28.27 < 2e-16 ***
o3_(Intercept)   0.9330     0.0868   10.75 < 2e-16 ***
o3_xo1           1.5339     0.0645   23.80 < 2e-16 ***
o3_xo2          -1.9654     0.0523  -37.56 < 2e-16 ***
sigma1           1.0283     0.0536   19.19 < 2e-16 ***
sigma2           1.1054     0.0356   31.07 < 2e-16 ***
sigma3           1.1838     0.0537   22.05 < 2e-16 ***
rho1             0.3249     0.0997    3.26  0.0011 ** 
rho2             0.2043     0.0770    2.65  0.0080 ** 
rho3             0.4122     0.0858    4.81 1.5e-06 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Wald chi2 (null): 4698 on 8 DF, p-value: < 0
Wald chi2 (rho): 40.8 on 3 DF, p-value: < 0
\end{Soutput}
\end{Schunk}
%
The presentation of the model results is fairly standard and should not warrant further explanation with the following exceptions
\begin{enumerate}
\item The number of regimes along absolute counts are reported.
\item Pseudo R-squared (EL) is based on the log-likelihood of the ``equally likely'' model, while Pseudo R-squared (MS) is based on the log-likelihood of the ``market share'' model. These indicators reflect the goodness of fit for the selection process. The multiple R-squared is reported for all continuous outcomes collectively and for the regimes separately in brackets. These indicators reflect the goodness of fit for the outcome processes.
\item Coefficient names are based on the variable names as passed to the formula specification, except that \code{"s_"} is prepended to the selection coefficients, \code{"o[0-9]_"} to the outcome coefficients and the structural components \code{"kappa", "sigma", "rho"} (aligning with the letters used in Equation~\ref{eq:log-lik}) are hard-coded (but can be over-written).
\item The coefficients table reports robust standard errors based on the sandwich covariance matrix as computed with help of the \pkg{sandwich} package \citep{Zeileis:2006}. \code{rob = FALSE} reports conventional standard errors.
\item Two Wald-tests are conducted. One, testing the null that all coefficients of explanatory variables are zero and two, testing the null that all error correlation coefficients (\code{rho}) are zero. The latter being rejected indicates that selection bias is an issue.
\end{enumerate}

A useful benchmark is always the null model with structural parameters only. The null model can be derived from an \class{opsr} model fit as follows
%
\begin{Schunk}
\begin{Sinput}
R> fit_null <- opsr_null_model(fit, printLevel = 0)
\end{Sinput}
\end{Schunk}
%
A model can be updated as usual
%
\begin{Schunk}
\begin{Sinput}
R> fit_intercept <- update(fit, . ~ . | 1)
\end{Sinput}
\end{Schunk}
%
where we have removed all the explanatory variables from the outcome processes.

Several models can be compared with a likelihood-ratio test using
%
\begin{Schunk}
\begin{Sinput}
R> anova(fit_null, fit_intercept, fit)
\end{Sinput}
\begin{Soutput}
Likelihood Ratio Test

Model 1: ~Nullmodel
Model 2: ys | yo ~ xs1 + xs2 | 1
Model 3: ys | yo ~ xs1 + xs2 | xo1 + xo2
  logLik    Df  Test Restrictions Pr(>Chi)    
1  -3302     8                                
2  -2798    13  1008            5   <2e-16 ***
3  -2015    19  1565            6   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{Soutput}
\end{Schunk}
%
If only a single object is passed, then the model is compared to the null model. If more than one object is specified a likelihood ratio test is conducted for each pair of neighboring models. As expected, both tests reject the null.

Models can be compared side-by-side using the \pkg{texreg} package \citep{Leifeld:2013}, which also allows the user to build production-grade tables as illustrated later.
%
\begin{Schunk}
\begin{Sinput}
R> texreg::screenreg(list(fit_null, fit_intercept, fit),
+    include.pseudoR2 = TRUE, include.R2 = TRUE, single.row = TRUE)
\end{Sinput}
\begin{Soutput}
==============================================================================
                 Model 1              Model 2              Model 3            
------------------------------------------------------------------------------
kappa1              -0.99 (0.05) ***     -2.13 (0.10) ***     -2.13 (0.10) ***
kappa2               0.41 (0.04) ***      0.88 (0.06) ***      0.88 (0.06) ***
sigma1               2.60 (0.16) ***      2.60 (0.16) ***      1.03 (0.05) ***
sigma2               2.15 (0.07) ***      2.15 (0.07) ***      1.11 (0.04) ***
sigma3               2.70 (0.10) ***      2.70 (0.10) ***      1.18 (0.05) ***
rho1                                      0.07 (0.14)          0.32 (0.10) ** 
rho2                                      0.13 (0.07)          0.20 (0.08) ** 
rho3                                      0.11 (0.11)          0.41 (0.09) ***
s_xs1                                     1.01 (0.06) ***      1.00 (0.06) ***
s_xs2                                     1.57 (0.07) ***      1.57 (0.07) ***
o1_(Intercept)       0.91 (0.21) ***      1.04 (0.30) ***      1.11 (0.10) ***
o1_xo1                                                         1.92 (0.07) ***
o1_xo2                                                         1.30 (0.08) ***
o2_(Intercept)       1.02 (0.10) ***      1.05 (0.10) ***      1.08 (0.05) ***
o2_xo1                                                        -1.05 (0.05) ***
o2_xo2                                                         1.49 (0.05) ***
o3_(Intercept)       1.10 (0.15) ***      0.95 (0.21) ***      0.93 (0.09) ***
o3_xo1                                                         1.53 (0.06) ***
o3_xo2                                                        -1.97 (0.05) ***
------------------------------------------------------------------------------
AIC               6619.01              5621.32              4067.99           
BIC               6658.27              5685.12              4161.24           
Log Likelihood   -3301.50             -2797.66             -2015.00           
Pseudo R^2 (EL)      0.08                 0.54                 0.54           
Pseudo R^2 (MS)     -0.00                 0.50                 0.50           
R^2 (total)          0.00                 0.00                 0.80           
R^2 (1)              0.00                 0.00                 0.85           
R^2 (2)              0.00                 0.01                 0.74           
R^2 (3)              0.00                 0.00                 0.82           
Num. obs.         1000                 1000                 1000              
==============================================================================
*** p < 0.001; ** p < 0.01; * p < 0.05
\end{Soutput}
\end{Schunk}
%
Finally, the key interest of an OPSR study almost certainly is the estimation of treatment effects which relies on (counterfactual) conditional expectations as already noted in the mathematical exposition.
%
\begin{Schunk}
\begin{Sinput}
R> p1 <- predict(fit, group = 1, type = "response")
R> p2 <- predict(fit, group = 1, counterfact = 2, type = "response")
\end{Sinput}
\end{Schunk}
%
where \code{p1} is the result of applying Equation~\ref{eq:cond-exp} and \code{p2} is the counterfactual outcome resulting from Equation~\ref{eq:counterfact-exp}. The following \code{type} arguments are available
\begin{itemize}
\item \code{type = "response"}: Predicts the continuous outcome according to the Equations referenced above.
\item \code{type = "unlog-response"}: Predicts the back-transformed response if the continuous outcome was log-transformed according to Equations~\ref{eq:log-cond-exp}-\ref{eq:log-counterfact-exp}.
\item \code{type = "prob"}: Returns the probability vector of belonging to \code{group}.
\item \code{type = "mills"}: Returns the inverse Mills ratio.
\end{itemize}
Elements are \code{NA_real_} if the \code{group} does not correspond to the observed regime (selection outcome). This ensures consistent output length.

Now that the user understands the basic workflow, we illustrate some nuances by reproducing a key output of \cite{Wang+Mokhtarian:2024} where they investigate the treatment effect of telework (TW) on weekly vehicle miles driven. The data is attached, documented (\code{?telework_data}) and can be loaded by
%
\begin{Schunk}
\begin{Sinput}
R> data("telework_data", package = "OPSR")
\end{Sinput}
\end{Schunk}
%
%
The final model specification reads
%
\begin{Schunk}
\begin{Sinput}
R> f <-
+    twing_status | vmd_ln ~
+    edu_2 + edu_3 + hhincome_2 + hhincome_3 + flex_work + work_fulltime +
+    twing_feasibility + att_proactivemode + att_procarowning + att_wif +
+    att_proteamwork + att_tw_effective_teamwork + att_tw_enthusiasm +
+    att_tw_location_flex |
+    female + age_mean + age_mean_sq + race_black + race_other + vehicle +
+    suburban + smalltown + rural + work_fulltime + att_prolargehouse +
+    att_procarowning + region_waa |
+    edu_2 + edu_3 + suburban + smalltown + rural + work_fulltime +
+    att_prolargehouse + att_proactivemode + att_procarowning |
+    female + hhincome_2 + hhincome_3 + child + suburban + smalltown +
+    rural + att_procarowning + region_waa
\end{Sinput}
\end{Schunk}
%
and the model can be estimated by
%
\begin{Schunk}
\begin{Sinput}
R> start_default <- opsr(f, telework_data, .get2step = TRUE)
R> fit <- opsr(f, telework_data, start = start, method = "NM", iterlim = 50e3,
+    printLevel = 0)
\end{Sinput}
\end{Schunk}
%
where we demonstrate that
\begin{enumerate}
\item Default starting values as computed by the Heckman two-step procedure can be retrieved.
\item \code{start} values can be overridden (we have hidden the \code{start} vector here for brevity). If the user wishes to pass start values manually, some minimal conventions have to be followed as documented in \code{?opsr_check_start}.
\item Alternative maximization methods (here ``Nelder-Mead'') can be used (as in the original paper).
\end{enumerate}
%
%
With help of the \pkg{texreg} package, production-grade tables (in various output formats) can be generated with ease.
%
\begin{Schunk}
\begin{Sinput}
R> texreg::texreg(
+    fit, beside = TRUE, include.structural = FALSE, include.R2 = TRUE,
+    include.pseudoR2 = TRUE, custom.model.names = custom.model.names,
+    custom.coef.names = custom.coef.names, reorder.coef = reorder.coef,
+    groups = groups, scalebox = 0.83, booktabs = TRUE, dcolumn = TRUE,
+    use.packages = FALSE, float.pos = "t!", single.row = TRUE,
+    caption = "Replica of \\cite{Wang+Mokhtarian:2024}, Table 3.",
+    label = "tab:wang-replica"
+  )
\end{Sinput}
\begin{table}[t!]
\begin{center}
\scalebox{0.83}{
\begin{tabular}{l D{)}{)}{9)3} D{)}{)}{9)3} D{)}{)}{9)3} D{)}{)}{9)3}}
\toprule
 & \multicolumn{1}{c}{Selection} & \multicolumn{1}{c}{NTWer (535)} & \multicolumn{1}{c}{NUTWer (322)} & \multicolumn{1}{c}{UTWer (727)} \\
\midrule
Education (ref: high school or less)       &                      &                      &                      &                       \\
\quad Some college                         & 0.32 \; (0.14)^{*}   &                      & 0.15 \; (0.33)       &                       \\
\quad Bachelor's degree or higher          & 0.47 \; (0.13)^{***} &                      & 0.62 \; (0.32)^{*}   &                       \\
Household income (ref: less than \$50,000) &                      &                      &                      &                       \\
\quad \$50,000 to \$99,999                 & 0.06 \; (0.12)       &                      &                      & 0.47 \; (0.23)^{*}    \\
\quad \$100,000 or more                    & 0.25 \; (0.11)^{*}   &                      &                      & 0.31 \; (0.23)        \\
Flexible work schedule                     & 0.31 \; (0.10)^{**}  &                      &                      &                       \\
Full time worker                           & 0.33 \; (0.10)^{**}  & 0.45 \; (0.13)^{***} & 0.69 \; (0.17)^{***} &                       \\
Teleworking feasibility                    & 0.13 \; (0.01)^{***} &                      &                      &                       \\
Attitudes                                  &                      &                      &                      &                       \\
\quad Pro-active-mode                      & 0.08 \; (0.04)^{*}   &                      & -0.18 \; (0.08)^{*}  &                       \\
\quad Pro-car-owning                       & -0.08 \; (0.04)^{*}  & 0.14 \; (0.07)^{*}   & 0.16 \; (0.09)       & 0.25 \; (0.06)^{***}  \\
\quad Work interferes with family          & 0.11 \; (0.04)^{**}  &                      &                      &                       \\
\quad Pro-teamwork                         & 0.09 \; (0.04)^{*}   &                      &                      &                       \\
\quad TW effective teamwork                & 0.32 \; (0.04)^{***} &                      &                      &                       \\
\quad TW enthusiasm                        & 0.09 \; (0.04)^{*}   &                      &                      &                       \\
\quad TW location flexibility              & 0.08 \; (0.04)^{*}   &                      &                      &                       \\
\quad Pro-large-house                      &                      & 0.18 \; (0.05)^{***} & 0.18 \; (0.08)^{*}   &                       \\
Intercept                                  &                      & 3.64 \; (0.27)^{***} & 2.49 \; (0.37)^{***} & 2.38 \; (0.26)^{***}  \\
Female                                     &                      & -0.21 \; (0.10)^{*}  &                      & -0.36 \; (0.11)^{***} \\
Age                                        &                      & 0.01 \; (0.00)^{*}   &                      &                       \\
Age squared                                &                      & -0.00 \; (0.00)      &                      &                       \\
Race (ref: white)                          &                      &                      &                      &                       \\
\quad Black                                &                      & -0.40 \; (0.24)      &                      &                       \\
\quad Other races                          &                      & -0.06 \; (0.18)      &                      &                       \\
Number of vehicles                         &                      & 0.12 \; (0.05)^{*}   &                      &                       \\
Residential location (ref: urban)          &                      &                      &                      &                       \\
\quad Suburban                             &                      & 0.07 \; (0.15)       & 0.45 \; (0.17)^{**}  & 0.28 \; (0.14)^{*}    \\
\quad Small town                           &                      & 0.47 \; (0.18)^{**}  & 0.19 \; (0.29)       & 0.29 \; (0.28)        \\
\quad Rural                                &                      & 0.60 \; (0.23)^{**}  & 0.81 \; (0.31)^{**}  & 0.88 \; (0.34)^{**}   \\
Region indicator (WAA)                     &                      & -0.25 \; (0.11)^{*}  &                      & -0.27 \; (0.11)^{*}   \\
Number of children                         &                      &                      &                      & 0.18 \; (0.06)^{**}   \\
\midrule
AIC                                        & 7191.35              & 7191.35              & 7191.35              & 7191.35               \\
BIC                                        & 7491.94              & 7491.94              & 7491.94              & 7491.94               \\
Log Likelihood                             & -3539.67             & -3539.67             & -3539.67             & -3539.67              \\
Pseudo R$^2$ (EL)                          & 0.49                 & 0.49                 & 0.49                 & 0.49                  \\
Pseudo R$^2$ (MS)                          & 0.46                 & 0.46                 & 0.46                 & 0.46                  \\
R$^2$ (total)                              & 0.24                 & 0.24                 & 0.24                 & 0.24                  \\
R$^2$ (1)                                  & 0.28                 & 0.28                 & 0.28                 & 0.28                  \\
R$^2$ (2)                                  & 0.23                 & 0.23                 & 0.23                 & 0.23                  \\
R$^2$ (3)                                  & 0.21                 & 0.21                 & 0.21                 & 0.21                  \\
Num. obs.                                  & 1584                 & 1584                 & 1584                 & 1584                  \\
\bottomrule
\multicolumn{5}{l}{\scriptsize{$^{***}p<0.001$; $^{**}p<0.01$; $^{*}p<0.05$}}
\end{tabular}
}
\caption{Replica of \cite{Wang+Mokhtarian:2024}, Table 3.}
\label{tab:wang-replica}
\end{center}
\end{table}\end{Schunk}
%
Dot arguments (\code{...}) passed to \fct{texreg} (or similar functions) are forwarded to a \proglang{S4} method \fct{extract} which extracts the variables of interest from a model fit (see also \code{?extract.opsr}). We demonstrate here that
\begin{enumerate}
\item Model components can be omitted. Here, the structural coefficients (\code{kappa}, \code{sigma}, \code{rho}) are disgarded (\code{include.structural = FALSE}).
\item The model components can be printed side-by-side (\code{beside = TRUE}).
\item Additional goodness-of-fit indicators can be included (\code{include.R2 = TRUE} and \code{include.pseudoR2 = TRUE}). Note that the indicators are repeated for all the model components.
\item The output formatting can be controlled flexibly, by reordering, renaming and grouping coefficients (the fiddly but trivial details are hidden here for brevity).
\end{enumerate}


%% -- Case study ---------------------------------------------------------------
\section{Case study} \label{sec:case-study}

Now, that the reader is familiar with the main functionality of \pkg{OPSR}, this section demonstrates how to employ it in a real-world example. The emphasis, therefore, lies not on what each function does but on guiding the reader through the modeling and post-estimation steps. We investigate once again, telework treatment effects on weekly distance traveled (aggregated over all modes of transport).

We first discuss the model building strategy to arrive at an appropriately specified OPSR model. We then demonstrate, why error correlation occurs, having omitted a variable simultaneously influencing the selection and outcome process. The OPSR models are compared to models not accounting for this error correlation and implications for treatment effects are shown. The case study concludes with a discussion on unit treatment effects investigating to what degree foregone commutes (when teleworking) are compensated with leisure travel.

%% The data
We use the TimeUse+ dataset \citep{Winkler+Meister+Axhausen:2024}, a smartphone-based diary, recording travel, time use, and expenditure data. Our analytical sample comprises employed individuals and is based on what \citet{Winkler+Axhausen:2024} identified as valid days. A valid day has at least 20 hours of information where 70\% of the events were validated by the user. Users who did not have at least 14 valid days were excluded. For the remaining 824 participants mobility indicators for a typical week were constructed. The telework status is based on tracked (and labelled) work activities and three regimes are differentiated: Non-teleworkers (NTW), Non-usual teleworkers (NUTW; $<$3 days/week) and Usual teleworkers (UTW; 3$+$ days/week).

The data, underlying this analysis, is attached, documented (\code{?timeuse_data}) and can be loaded by
%
\begin{Schunk}
\begin{Sinput}
R> data("timeuse_data", package = "OPSR")
\end{Sinput}
\end{Schunk}
%
A basic boxplot of the response variable against the three telework statuses is displayed in Figure~\ref{fig:boxplot}. By simply looking at the data descriptively, we might prematurely conclude that telework does not impact weekly distance traveled. However, the whole value proposition of OPSR (and of models in general) is that we really are interested in a counterfactual. If the teleworkers self-select, the counterfactual is not simply the group average. More prosaically, if the usual telewokers (UTW) would choose to be non-teleworkers (NTW), they might travel more or less than the actual NTWers.

Meanwhile, commute distance increases across the three teleworker groups, suggesting that, one, longer commutes increase the propensity to telework and two, teleworkers have a higher share of leisure travel (given the similar overall distance traveled).

\setkeys{Gin}{width=.8\textwidth}
\begin{figure}[t!]
\centering
