\chapter{Estimation and post-estimation routines for OPSR}
\label{ch:opsr-tools}

\dictum[John Doe]{%
Hello world!}%
\vskip 1em

<<opsr-tools-preliminaries, echo=FALSE, results=hide>>=
colvec <- c("#4c5760","#ff8811","#48a9a6")
# scales::alpha(colvec, 0.3)
colvec_alpha <- c("#4C57604C", "#FF88114C", "#48A9A64C")

cache <- readRDS("./cache.rds")
fit <- cache$fit
fit_10 <- cache$fit_10
fit_aic <- cache$fit_aic
kfold <- cache$kfold
kfold_10 <- cache$kfold_10
kfold_aic <- cache$kfold_aic
@

%% -- Abstract -----------------------------------------------------------------

The \pkg{OPSR} \proglang{R}-package allows analysts to estimate ordered probit switching regression models, a form of endogenous switching regression, accounting for error correlation between an ordinal selection and continuous outcome processes (e.g., the well-known Tobit-5 model for the case of two regimes). \pkg{OPSRtools}, introduced in this paper, helps the analyst during model selection (\fct{opsr\_select}), to cross-validate (\fct{opsr\_kfold} and \fct{kfplot}), to compute treatment effects (\fct{opsr\_ate}) and to visualize the key insights from a model (\fct{pairs}). \fct{print} methods for the various underlying objects allow the analyst to easily grasp the object's key information.

\begin{ChapterInfoTable}
\ChapterInfoEntry{%
This chapter is based on the following paper
}{%
Heimgartner D. (2025) \pkg{OPSRtools}: Estimation and Post-Estimation Routines for the \pkg{OPSR} Package, unpublished work.
}%
\end{ChapterInfoTable}

%% -- Introduction -------------------------------------------------------------

\section{Introduction}

The \pkg{OPSR} \proglang{R}-package, intruduced in \citet{Heimgartner+Wang:2025}, provides an easy-to-use, fast and memory efficient interface to ordered probit switching regression (OPSR). OPSR is a form of endogenous switching regression, accounting for error correlation between an ordinal selection and continuous outcome processes. In spirit of \citet{Heckman:1979}, self-selection bias might result, if this error correlation is not explicitly modeled.

While the \pkg{OPSR} package contains the minimal machinery to estimate such models, present their results and compute conditional expectations, it does not provide routines supporting model selection, cross-validation and the computation of treatment and average treatment effects. This is where \pkg{OPSRtools} steps in. Along functions and methods to compute and present (\fct{print}) the statistics in appropriate form, it also contains visualization routines.

The remainder of this paper is organized as follows. In Section~\ref{sec:opsr-tools-illustrations} the key functionality of the package is illustrated. Section~\ref{sec:opsr-tools-identification} investigates differences in telework treatment effects if the error correlation is not accounted for as well as raises concerns about potential identification issues. Section~\ref{sec:opsr-tools-summary} concludes.

%% -- Illustrations ------------------------------------------------------------

\section{Illustrations} \label{sec:opsr-tools-illustrations}

Let us revisit the full model from Chapter~\ref{ch:twte}, modeling telework adoption and weekly kilometers traveled. Telework adoption was differentiated into three ordered segments: no telework (NTW), non-usual telework (NUTW, <3 days/week) and usual telework (UTW, 3+ days/week). The data, based on the TimeUse+ study \citep{Winkler+Meister+Axhausen:2024}, is attached, documented (\code{?timeuse}) and can be loaded by
%
<<opsr-tools-data>>=
data("timeuse", package = "OPSRtools")
@
%
Using \pkg{OPSR}'s functionality, the model can be estimated by (the formula \code{f} is hidden here for brevity but can be found on Page~\pageref{p:opsr-tools-f} under \emph{Initial model})
%
<<opsr-tools-hidden-formula, echo=FALSE>>=
f <- wfh | log_weekly_km ~ wfh_allowed + teleworkability + start_tracking +
  log_commute_km + age + dogs + driverlicense + educ_higher + fixed_workplace +
  grocery_shopper + hh_income + hh_size + isco_clerical + isco_craft +
  isco_managers + isco_plant + isco_professionals + isco_service + isco_agri +
  isco_tech + married + n_children + freq_onl_order + parking_home +
  parking_work + permanent_employed + rents_home + res_loc + sex_male +
  shift_work + swiss + vacation + workload + young_kids |
  start_tracking + log_commute_km + age + dogs + driverlicense + educ_higher +
  fixed_workplace + grocery_shopper + hh_income + hh_size + married +
  n_children + freq_onl_order + parking_home + parking_work +
  permanent_employed + rents_home + res_loc + sex_male + shift_work + swiss +
  vacation + workload + young_kids |
  start_tracking + log_commute_km + age +
  dogs + driverlicense + educ_higher + fixed_workplace + grocery_shopper +
  hh_income + hh_size + married + n_children + freq_onl_order + parking_home +
  parking_work + permanent_employed + rents_home + res_loc + sex_male +
  shift_work + swiss + vacation + workload + young_kids |
  start_tracking + log_commute_km + age + dogs + driverlicense + educ_higher +
  fixed_workplace + grocery_shopper + hh_income + hh_size + married +
  n_children + freq_onl_order + parking_work + permanent_employed + rents_home +
  res_loc + sex_male + swiss + vacation + workload + young_kids
@
%
<<opsr-tools-full-model, results=hide>>=
fit <- opsr(f, data = timeuse, weights = timeuse$weight)
@
%
This model is likely to be over-specified and as a usual first step, the analyst starts by excluding insignificant variables (above some $p$~value threshold). Given the above formula object with many terms/variables, this process can be tedious or even error prone - boring at best. The function \fct{opsr\_step} allows the modeler to do this automatically. For example, excluding all variables which are not significant at the 10\% level
%
<<opsr-tools-opsr-step>>=
fit_10 <- opsr_step(fit, pval = 0.1, printLevel = 0)
summary(fit_10)
@
%
where \code{printLevel = 0} is passed to \fct{opsr} and avoids printing working information during maximum likelihood estimation. \fct{opsr\_step} is aware of factor variables and keeps the factor if one corresponding coefficient is below the selected \code{pval}.

While this model (\fit{10}) has considerably fewer parameters than \code{fit}, it is not necessarily the ``best'' model. To more rigorously sparsify the model and carefully evaluate each intermediate model, the function \fct{opsr\_select} can be used
%
<<opsr-tools-opsr-select, eval=FALSE>>=
fit_aic <- opsr_select(fit, loss = "aic", printLevel = 0)
@
%
where \code{"aic"} specifies that model selection is based on AIC. Other available options are \code{"bic"} for BIC and \code{"lrt"} for a likelihood ratio test. The above call yields an object of class \class{opsr.select} which can be printed to provide more information about the selection process (and elimination history, which is omitted here for brevity)
\label{p:opsr-tools-f}
%
<<opsr-tools-print-opsr-select>>=
print(fit_aic, print.elim.hist = FALSE)
@
%
The final model (after the sixth iteration) has improved AIC by 49 points and would be preferred over \fit{10}. The model comparison table reads as follows
%
\begin{itemize}
\item Current winner: The currently best performing model.
\item Current: ``Loss'' value (here AIC) of the current winner.
\item Opponent: ``Loss'' value of the model at the current step.
\item Test: Test-statistic (here AIC difference).
\item Winner: Selected model based on the test-statistic (current winner in the next step).
\end{itemize}

The three models can be compared by
%
<<opsr-tools-anova>>=
print(anova(fit_10, fit_aic, fit), print.formula = FALSE)
@
%
where the likelihood ratio test would select the full model. However, the full model is potentially over-fitting the data (in particular for the usual teleworkers, UTWers). To further investigate the out-of-sample (OOS) performance, k-fold cross-validation can be performed
%
<<opsr-tools-opsr-kfold, eval=FALSE>>=
kfold <- opsr_kfold(fit, k = 10, verbose = FALSE, printLevel = 0)
kfold_10 <- opsr_kfold(fit_10, k = 10, verbose = FALSE, printLevel = 0)
kfold_aic <- opsr_kfold(fit_aic, k = 10, verbose = FALSE, printLevel = 0)
@
%
The resulting object of class \class{opsr.kfold} is a nested list: It contains for each fold different out-of-sample loss computations

\begin{itemize}
\item \code{z}: The regime/treatment membership (here telework status) of the test data.
\item \code{ll}: The OOS log-likelihood.
\item \code{ll\_mean}: \code{ll} averaged over each regime and in total.
\item \code{ll\_p}: The OOS log-likelihood for the selection process (ordered probit model).
\item \code{ll\_p\_mean}: \code{ll\_p} averaged over each regime and in total.
\item \code{r2}: The OOS coefficient of determination ($R^2$) for each regime and in total.
\end{itemize}

\pkg{OPSRtools} provides an extract method to collect the desired information
%
<<opsr-tools-extract>>=
ll_mean <- kfold["ll_mean"]
list2df(ll_mean)
@
%
The three cross-validation objects can be plotted against each other using \fct{kfplot}
<<opsr-tools-kfplot, eval=FALSE>>=
plot.it <- function(...) {
  op <- par(no.readonly = TRUE)
  on.exit(par(op))
  par(mfrow = c(1, 3))
  par(...)
  what <- c("ll_mean", "ll_p_mean", "r2")
  main <- c("Total", "Selection", "Outcome")
  ylab <- c("OOS log-likelihood full model",
            "OOS log-likelihood selection model",
            expression(paste("OOS ", R^2, "")))
  for (i in seq_along(what)) {
    kfplot(list(kfold_10, kfold_aic, kfold), i = what[i], col = colvec,
           main = main[i], ylab= ylab[i], xlab = "", las = 2)
    legend("bottomleft", legend = c("fit_10", "fit_aic", "fit"),
           fill = colvec, title = "Model", title.font = 2, bty = "n")
  }
}
plot.it()
@
%
In Figure~\ref{fig:opsr-tools-kfplot} three OOS losses (for the total model \code{ll\_mean}, the selection \code{ll\_p\_mean} and the continuous outcome \code{r2}) are presented: \fit{10} has negligible higher $R^2$ values for the UTWers but generally performs slightly worse than the other two models.

\setkeys{Gin}{width=\textwidth}
\begin{figure}[htbp]
\centering
<<opsr-tools-plot-kfplot, echo=FALSE, fig=TRUE, height=3.2, width=7.6>>=
<<opsr-tools-kfplot>>
@
\caption{\label{fig:opsr-tools-kfplot} Model comparison by k-fold cross-validation. Note that $R^2$ can be negative for out-of-sample data points.}
\end{figure}

Having selected an appropriate model (say, in the name of Occam's razor, \fit{10}) the analyst would like to investigate treatment effects (TE). This can be done by
%
<<opsr-tools-opsr-ate>>=
ate <- opsr_ate(fit_10, type = "unlog-response")
print(ate)
@
%
where we needed to specify \code{type = "unlog-response"} (which is passed to \fct{predict.opsr}) since the outcome variable was log-transformed (log weekly kilometers traveled) before estimation. \fct{opsr\_ate} prepares the data, returning an object of class \class{opsr.ate} and \fct{summary.opsr.ate} then performs the main computations, in particular a weighted paired $t$~test for the TE (with the null that the TE is equal to 0). The method \fct{print.summary.opsr.ate} (i.e., \fct{print} called on an object of class \class{summar.opsr.ate}) prints the results (method \fct{print.opsr.ate} redirects to \fct{summary} and a subsequent \fct{print} call).

Weights are automatically used to compute group-specific treatment effects and average treatment effects (ATE) if they were used during estimation but can be overridden by passing a new vector as \code{weights} argument to \fct{opsr\_ate}.

Here we see, that the TE on the diagonal and upper triangle are negative, while we observe positive TE for \code{G1} (the NTWers) when switching from \code{T1->T3} (NTWing to UTWing), \code{T2->T3} (NUTWing to UTWing) and \code{G2} (the NUTWers) when switching from \code{T2->T3} (NUTWing to UTWing). Consequently, resulting ATE are positive for \code{T1->T3} and \code{T2->T3}. All effects are significantly different from 0. However, the test does of course not reflect parameter uncertainty.

The treatment effects can be visually summarised by
%
<<opsr-tools-pairs, eval=FALSE>>=
pairs(ate)
@
%
Figure~\ref{fig:opsr-tools-pairs} presents all potential pairs of regime switching. The diagonal depicts distributions of model-implied conditional expectations in any given treatment and separate by the current (factual) treatment group. The weighted mean values are shown as red numbers. The lower triangular panels compare the model-implied (predicted) outcomes of two treatment regimes, again, separate by current treatment group. The red line indicates the 45-degree line of equal outcomes while the red squares depict the weighted mean values (corresponding to the reported numbers). The upper triangular panels show (weighted) average treatment effects.

\setkeys{Gin}{width=\textwidth}
\begin{figure}[htbp]
\centering
<<opsr-tools-plot-pairs, echo=FALSE, fig=TRUE, height=7.5, width=7.5>>=
pairs(ate, labels.diag = c("NTWing", "NUTWing", "UTWing"), col = colvec,
      labels.reg = c("NTWers", "NUTWers", "UTWers"), cex = 1.75, cex.labels = 2,
      font.labels = 2,lwd.dens = 2.5, lty.diag = 4, xlim = c(0, 1000),
      ylim = c(0, 1000))
@
\caption{\label{fig:opsr-tools-pairs} Pairs plot: Comparison of conditional expectations (weekly km traveled) by telework status in the lower panel. Corresponding distributions on the diagonal. Average treatment effects in the upper panel.}
\end{figure}

%% -- Identification -----------------------------------------------------------

\section{Investigating differences in treatment effects} \label{sec:opsr-tools-identification}

At this point, we should highlight, that these results (based on \fit{10}) are very different from the ones presented in Chapter~\ref{ch:twte} (based on \fit{aic}), where we found negative TE and ATE only. We now further investigate, why these differences between the two models might arise. The following three hypothesis are scrutinized: 1. The large error correlation ``rho3'' in \fit{10} could inflate (counterfactual) weekly kilometers traveled of the NTWers and NUTWers when adopting NUTWing or UTWing via the Heckman correction term, 2. \fit{10} has fewer explanatory variables than \fit{aic} and therefore $\Xb$ in the two models differ, and, 3. The $\beta$ coefficients are very different (and therefore $\Xb$ in the two models differ).

Let us first revisit the conditional expectation of the OPSR model as stated in \citet{Heimgartner+Wang:2025}
%
\begin{equation} \label{eq:opsr-tools-cond-exp}
\E[y_j \mid Z = j] = \underbrace{\Xb}_{{\text{\code{type = "Xb"}}}} - \underbrace{\rho_j\sigma_j \frac{\phi(\kappa_j - \Wg) - \phi(\kappa_{j-1} - \Wg)}{\Phi(\kappa_j - \Wg) - \Phi(\kappa_{j-1} - \Wg)}}_{{\text{\code{type = "correction"}}}},
\end{equation}
%
where $\Xb$ is the linear combination underlying the continuous outcome, $\kappa_j$ and $\kappa_{j-1}$ are the thresholds of the ordered probit model, $\Wg$ is the linear combination modeling the latent propensity underlying the selection outcome, $\phi(\cdot)$ and $\Phi(\cdot)$ are the density and cumulative distribution function of the standard normal distribution. The fraction is the ordered probit switching regression model counterpart to the inverse Mills ratio (IMR) term of a binary switching regression model and the leading cause of selection bias. We also see, that higher $\rho_j$ (and/or higher $\sigma_j$) ``inflates'' the Heckman correction by scaling the IMR.

Similarly, the ``counterfactual outcome'' reflects the expected outcome under a counterfactual treatment (i.e., for $j' \neq j$) and can be expressed as
%
\begin{equation} \label{eq:opsr-tools-counterfact-exp}
\E[y_{j'} \mid Z = j] = \Xbd - \rho_{j'}\sigma_{j'} \frac{\phi(\kappa_j - \Wg) - \phi(\kappa_{j-1} - \Wg)}{\Phi(\kappa_j - \Wg) - \Phi(\kappa_{j-1} - \Wg)}.
\end{equation}
%
\pkg{OPSR}'s \fct{predict} method can return both components by specifying \code{type = "Xb"} respectively \code{type = "correction"} (as underbraced in Equation~\ref{eq:opsr-tools-cond-exp}). Since the dependent variable was log-transformed, the above formulas return the conditional expectation in the logs. The equations for the back-transformed conditional expectations can be found in \citet{Heimgartner+Wang:2025} and are derived in \citet{Wang+Mokhtarian:2024}. However, in these equations, the decomposition into linear predictor and Heckman correction is no longer evident.

\setkeys{Gin}{width=\textwidth}
\begin{figure}[htbp]
\centering
<<opsr-tools-heckman-correction, echo=FALSE, fig=TRUE, height=7.5, width=7>>=
heckman_ij <- function(fit, i, j) {
  correction <- predict(fit, type = "correction", group = i, counterfact = j)
  correction
}

xb_ij <- function(fit, i, j) {
  xb <- predict(fit, type = "Xb", group = i, counterfact = j)
  xb
}

plot.values <- function(fit, i, j, digits = 3) {
  h <- heckman_ij(fit, i, j)
  x <- xb_ij(fit, i, j)
  mh <- mean(h, na.rm = TRUE)
  mx <- mean(x, na.rm = TRUE)
  ratio <- round(mh / mx, digits = digits)
  list(h = h, x = x, mh = mh, mx = mx, ratio = ratio)
}

plot.it <- function(fit1, fit2) {
  op <- par(no.readonly = TRUE)
  on.exit(par(op))
  par(mfrow = c(3, 3))

  group <- c("NTWers", "NUTWers", "UTWers")
  counterfact <- c("NTWing", "NUTWing", "UTWing")

  for (i in 1:3) {
    for (j in 1:3) {
      pv1 <- plot.values(fit1, i, j)
      pv2 <- plot.values(fit2, i, j)
      plot(pv1$h, pv1$x, xlim = c(-0.7, 0.7), ylim = c(3, 7),
           xlab = expression(rho[j]*sigma[j]*IMR), ylab = expression(X*beta),
           main = paste(group[i], counterfact[j]), type = "n")
      points(pv1$h, pv1$x, col = colvec_alpha[1], pch = 19)
      points(pv2$h, pv2$x, col = colvec_alpha[2], pch = 19)
      abline(v = pv1$mh, col = colvec[1], lty = 2, lwd = 2)
      abline(h = pv1$mx, col = colvec[1], lty = 2, lwd = 2)
      abline(v = pv2$mh, col = colvec[2], lty = 2, lwd = 2)
      abline(h = pv2$mx, col = colvec[2], lty = 2, lwd = 2)
      points(x = pv1$mh, y = pv1$mx, pch = 15)
      points(x = pv2$mh, y = pv2$mx, pch = 15)
      text(x = pv1$mh, y = pv1$mx, labels = pv1$ratio, adj = c(-0.1, -0.1), font = 2)
      text(x = pv2$mh, y = pv2$mx, labels = pv2$ratio, adj = c(1.1, 1.1), font = 2)
      legend("bottomleft", legend = c(substitute(fit1), substitute(fit2)), pch = 19,
             col = colvec[c(1, 2)], bty = "n")
    }
  }
}

plot.it(fit_10, fit_aic)
@
\caption{\label{fig:opsr-tools-heckman-correction} Plotting the Heckman correction term against the linear predictor. The ratio of the two mean values is printed at their intersection. Scales are shared across the facets.}
\end{figure}

In Figure~\ref{fig:opsr-tools-heckman-correction} we plot the Heckman correction term against the linear predictor $\Xb$ respectively $\Xbd$. The ratio of the two mean values is printed at their intersection. We see, that 1. the correction term is small compared to $\Xb$ and 2. prediction differences (between \fit{aic} and \fit{10}) are because of differences in $\Xb$: In the last column, the horizontal dashed lines corresponding to \fit{10} are above the ones of \fit{aic} and in particular above the level of the lines in column one and two which yields the positive treatment effects oberseved in Figure~\ref{fig:opsr-tools-pairs}.

The main motivation of OPSR is to correct for biased parameter estimates. We therefore compare the predictions of conventional regression models (i.e., fixing the ``rho'' coefficients at 0 during estimation) to the OPSR models.

\setkeys{Gin}{width=\textwidth}
\begin{figure}[htbp]
\centering
<<opsr-tools-fixing-rho, echo=FALSE, fig=TRUE, height=7.5, width=7>>=
start <- coef(fit_aic)
fixed <- c("rho1", "rho2", "rho3")
start[fixed] <- 0

fit_aic_nocor <- opsr(fit_aic$formula, timeuse, weights = timeuse$weight,
                      start = start, fixed = fixed, printLevel = 0)

start <- coef(fit_10)
fixed <- c("rho1", "rho2", "rho3")
start[fixed] <- 0

fit_10_nocor <- opsr(fit_10$formula, timeuse, weights = timeuse$weight,
                     start = start, fixed = fixed, printLevel = 0)

plot.values <- function(fit, i, j) {
  p <- predict(fit, group = i, counterfact = j, type = "unlog-response")
}

plot.it <- function() {
  op <- par(no.readonly = TRUE)
  on.exit(par(op))
  par(mfrow = c(3, 3))

  group <- c("NTWers", "NUTWers", "UTWers")
  counterfact <- c("NTWing", "NUTWing", "UTWing")

  for (i in 1:3) {
    for (j in 1:3) {
      x1 <- plot.values(fit_10, i, j)
      y1 <- plot.values(fit_10_nocor, i, j)
      x2 <- plot.values(fit_aic, i, j)
      y2 <- plot.values(fit_aic_nocor, i, j)
      plot(x1, y1, xlim = c(0, 800), ylim = c(0, 800),
           xlab = "Prediction (OPSR)", ylab = "Prediction (OLS)",
           main = paste(group[i], counterfact[j]), type = "n")
      points(x1, y1, col = colvec_alpha[1], pch = 19)
      points(x2, y2, col = colvec_alpha[2], pch = 19)
      abline(a = 0, b = 1, col = "red", lwd = 2, lty = 4)
      legend("bottomright", legend = c("fit_10", "fit_aic"), pch = 19,
             col = colvec[c(1, 2)], bty = "n")
    }
  }
}

plot.it()
@
\caption{\label{fig:opsr-tools-fixing-rho} The influence of accounting for error correlation on predictions: Plotting the OPSR predictions against the OLS predictions.}
\end{figure}

Figure~\ref{fig:opsr-tools-fixing-rho} compares the OPSR predictions (accounting for self-selection) to the OLS predictions (not accounting for self-selection) for both \fit{10} and \fit{aic}. For example, the first panel (NTWers NTWing) shows, that accounting for self-selection does not influence the prediction results, while panel UTWers NUTWing (row 3, column 2) indicates, that there is a selection bias, underestimating the weekly kilometers traveled of UTWers when NUTWing. Meanwhile, the selection bias is comparable for \fit{10} and \fit{aic}. Interestingly, the two models seem to agree on the selection bias with the exception of NTWers UTWing and NUTWers UTWing (column 3, row 1 and 2), when \fit{10} suddenly suggests a sizable selection bias - considerably underestimating the weekly distance traveled if not accounted for. The treatment effects of the four models are summarized in Table~\ref{tab:opsr-tools-te-comparison} where the reader can confirm, that resulting treatment effects are very different. However, the OLS model \fit{aic\_nocor} (i.e., the model with the same specification $\Xb$ as \fit{aic}) agrees with the direction of the treatment effect, despite generally underestimating it. Similarly, \fit{10\_nocor} shows negative TE and ATE and the numbers are similar to \fit{aic\_nocor}. Again, the reversed TE for \fit{10} seem to be the clear exception.

\begin{table}[htbp]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{llrrrr}
\toprule
Model & Regime switch & NTWers & NUTWers & UTWers & ATE \\
\midrule
<<opsr-tools-te-comparison, echo=FALSE, results=tex>>=
te_table <- function(fit) {
  ate <- opsr_ate(fit, type = "unlog-response")
  sry_ate <- summary(ate)
  te <- cbind(sry_ate$te, ATE = sry_ate$ate)
  te
}

models <- list(fit_aic, fit_aic_nocor, fit_10, fit_10_nocor)
te <- format(Reduce(rbind, lapply(models, te_table)), digits = 5)
from_to <- c("NTWing$\\rightarrow$NUTWing",
             "NTWing$\\rightarrow$UTWing",
             "NUTWing$\\rightarrow$UTWing")
space <- rep("", 2)
mat <- cbind(
  c("\\code{fit\\_aic}", space,
    "\\code{fit\\_aic\\_nocor}", space,
    "\\code{fit\\_10}", space,
    "\\code{fit\\_10\\_nocor}", space),
  c(rep(from_to, 4)),
  te
)
mat_tex <- paste0(apply(mat, 1, paste, collapse = " & "), "\\\\")
mat_tex[c(3, 6, 9)] <- paste(mat_tex[c(3, 6, 9)], "\\addlinespace")
writeLines(mat_tex)
@
\bottomrule
\end{tabular}
}%
\caption{\label{tab:opsr-tools-te-comparison} Treatment effect comparison. \code{nocor} implies the regular OLS fit with ``rho'' fixed at 0.}
\end{table}

This evidence suggests, that it is not actually the direct influence of the error correlation via the Heckman correction term but the indirect influence via the $\beta$ coefficients that leverages the predictions and thus treatment effect computations. And if we only have few explanatory variables, then the error correlation might have a bigger influence on these estimates. Under certain conditions, OPSR models might become poorly identified, suggesting extreme selection bias.

Identification issues of Heckman-type selection models have received considerable attention in the literature \citep[e.g.,][]{Puhani:2000}, investigating conditions under which model estimation becomes challenging. In Monte Carlo simulation studies, the performance of LIML and FIML estimators are then usually compared. The main condition discussed is weak exclusion-restrictions (weak instruments; leading to colinearity between the IMR and $\boldsymbol{X}$) as elaborated in \citet{Chiburis+Lokshin:2007}. \citet{Huismans+etal:2022} discuss maximum likelihood estimation and parameter identification (for a Heckman-selection model with ordered outcomes) and state that in mixture models, the likelihood function may have multiple local maximums and large flat regions and suggest testing different starting values (in our case, both \fit{10} and \fit{aic} converge to the same optimum if we set starting values to 0, -1 for ``kappa1'', 1 for ``kappa2'' and 1 for the ``sigmas''). Since the IMR is technically a regressor, we speculate, that the limited variability (as observed in Figure~\ref{fig:opsr-tools-heckman-correction}, in particular for UTWing, column 3) could also yield poor identification.

%% -- Summary ------------------------------------------------------------------

\section{Summary} \label{sec:opsr-tools-summary}

\pkg{OPSRtools} introduced in this article contains estimation and post-estimation routines for the \pkg{OPSR} \proglang{R}-package, allowing modelers to efficiently specify, select and evaluate ordered probit switching regression models. Treatment effects are easily computed, summarised and visualized. We further investigated model-implied differences in treatment effects and why they might arise. Visualizations are proposed to scrutinize the influence of the Heckman correction term on predictions and thus treatment effects. However, we find that in our case study, treatment effects do not mainly differ because of this correction but because OPSR yields different (and hopefully unbiased) parameter estimates. We also raise concerns about the stability of such models and potential identification issues. It is left for future research to propose methods detecting poor identification and its leading cause.
